nccl timeout value is set as 3600s!
2026-01-14 04:35:04,328 - mmdet - INFO - Environment info:
------------------------------------------------------------
MMCV: 0.0.1
------------------------------------------------------------

2026-01-14 04:35:07,151 - mmdet - INFO - Distributed training: True
2026-01-14 04:35:09,667 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
    'traffic_light', 'pedestrian', 'others'
]
dataset_type = 'B2DOrionDataset'
data_root = 'data/bench2drive'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=True,
        with_light_state=True),
    dict(
        type='VADObjectRangeFilter',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='VADObjectNameFilter',
        classes=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ]),
    dict(
        type='LoadAnnoatationVQA',
        base_desc_path=None,
        tokenizer='ckpts/pretrain_qformer/',
        max_length=2048,
        use_gen_token=True,
        planning_qa_only=True,
        planning_qa_last=True),
    dict(
        type='ResizeCropFlipRotImage',
        data_aug_conf=dict(
            resize_lim=(0.37, 0.45),
            final_dim=(320, 640),
            bot_pct_lim=(0.0, 0.0),
            rot_lim=(0.0, 0.0),
            H=900,
            W=1600,
            rand_flip=False),
        training=True),
    dict(
        type='ResizeMultiview3D',
        img_scale=(640, 640),
        keep_ratio=False,
        multiscale_mode='value'),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='NormalizeMultiviewImage',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(
        type='PETRFormatBundle3D',
        class_names=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ],
        collect_keys=[
            'lidar2img', 'cam_intrinsic', 'timestamp', 'ego_pose',
            'ego_pose_inv', 'command'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'img', 'ego_his_trajs',
            'input_ids', 'gt_attr_labels', 'ego_fut_trajs', 'ego_fut_masks',
            'ego_fut_cmd', 'ego_lcf_feat', 'vlm_labels', 'can_bus',
            'traffic_state_mask', 'traffic_state', 'lidar2img',
            'cam_intrinsic', 'timestamp', 'ego_pose', 'ego_pose_inv', 'command'
        ])
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
    dict(
        type='LoadAnnotations3D',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=True),
    dict(
        type='VADObjectRangeFilter',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='VADObjectNameFilter',
        classes=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ]),
    dict(
        type='ResizeCropFlipRotImage',
        data_aug_conf=dict(
            resize_lim=(0.37, 0.45),
            final_dim=(320, 640),
            bot_pct_lim=(0.0, 0.0),
            rot_lim=(0.0, 0.0),
            H=900,
            W=1600,
            rand_flip=False),
        training=False),
    dict(
        type='ResizeMultiview3D',
        img_scale=(640, 640),
        keep_ratio=False,
        multiscale_mode='value'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnoatationCriticalVQATest',
        load_type=['critical_qa'],
        tokenizer='ckpts/pretrain_qformer/',
        use_gen_token=True,
        max_length=2048),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='PETRFormatBundle3D',
                collect_keys=[
                    'lidar2img', 'cam_intrinsic', 'timestamp', 'ego_pose',
                    'ego_pose_inv', 'command'
                ],
                class_names=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'img', 'ego_his_trajs',
                    'input_ids', 'gt_attr_labels', 'ego_fut_trajs',
                    'ego_fut_masks', 'ego_fut_cmd', 'ego_lcf_feat',
                    'vlm_labels', 'can_bus', 'fut_valid_flag', 'lidar2img',
                    'cam_intrinsic', 'timestamp', 'ego_pose', 'ego_pose_inv',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=4,
    workers_per_gpu=4,
    train=dict(
        type='B2DOrionDataset',
        data_root='data/bench2drive',
        ann_file='data/infos/b2d_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=True,
                with_light_state=True),
            dict(
                type='VADObjectRangeFilter',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='VADObjectNameFilter',
                classes=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ]),
            dict(
                type='LoadAnnoatationVQA',
                base_desc_path=None,
                tokenizer='ckpts/pretrain_qformer/',
                max_length=2048,
                use_gen_token=True,
                planning_qa_only=True,
                planning_qa_last=True),
            dict(
                type='ResizeCropFlipRotImage',
                data_aug_conf=dict(
                    resize_lim=(0.37, 0.45),
                    final_dim=(320, 640),
                    bot_pct_lim=(0.0, 0.0),
                    rot_lim=(0.0, 0.0),
                    H=900,
                    W=1600,
                    rand_flip=False),
                training=True),
            dict(
                type='ResizeMultiview3D',
                img_scale=(640, 640),
                keep_ratio=False,
                multiscale_mode='value'),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(
                type='PETRFormatBundle3D',
                class_names=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ],
                collect_keys=[
                    'lidar2img', 'cam_intrinsic', 'timestamp', 'ego_pose',
                    'ego_pose_inv', 'command'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'img', 'ego_his_trajs',
                    'input_ids', 'gt_attr_labels', 'ego_fut_trajs',
                    'ego_fut_masks', 'ego_fut_cmd', 'ego_lcf_feat',
                    'vlm_labels', 'can_bus', 'traffic_state_mask',
                    'traffic_state', 'lidar2img', 'cam_intrinsic', 'timestamp',
                    'ego_pose', 'ego_pose_inv', 'command'
                ])
        ],
        classes=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        seq_mode=True,
        seq_split_num=1,
        name_mapping=dict({
            'vehicle.bh.crossbike':
            'bicycle',
            'vehicle.diamondback.century':
            'bicycle',
            'vehicle.gazelle.omafiets':
            'bicycle',
            'vehicle.audi.etron':
            'car',
            'vehicle.chevrolet.impala':
            'car',
            'vehicle.dodge.charger_2020':
            'car',
            'vehicle.dodge.charger_police':
            'car',
            'vehicle.dodge.charger_police_2020':
            'car',
            'vehicle.lincoln.mkz_2017':
            'car',
            'vehicle.lincoln.mkz_2020':
            'car',
            'vehicle.mini.cooper_s_2021':
            'car',
            'vehicle.mercedes.coupe_2020':
            'car',
            'vehicle.ford.mustang':
            'car',
            'vehicle.nissan.patrol_2021':
            'car',
            'vehicle.audi.tt':
            'car',
            'vehicle.ford.crown':
            'car',
            'vehicle.tesla.model3':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/FordCrown/SM_FordCrown_parked.SM_FordCrown_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Charger/SM_ChargerParked.SM_ChargerParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Lincoln/SM_LincolnParked.SM_LincolnParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/MercedesCCC/SM_MercedesCCC_Parked.SM_MercedesCCC_Parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Mini2021/SM_Mini2021_parked.SM_Mini2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/NissanPatrol2021/SM_NissanPatrol2021_parked.SM_NissanPatrol2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/TeslaM3/SM_TeslaM3_parked.SM_TeslaM3_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/VolkswagenT2/SM_VolkswagenT2_2021_Parked.SM_VolkswagenT2_2021_Parked':
            'van',
            'vehicle.ford.ambulance':
            'van',
            'vehicle.carlamotors.firetruck':
            'truck',
            'traffic.speed_limit.30':
            'traffic_sign',
            'traffic.speed_limit.40':
            'traffic_sign',
            'traffic.speed_limit.50':
            'traffic_sign',
            'traffic.speed_limit.60':
            'traffic_sign',
            'traffic.speed_limit.90':
            'traffic_sign',
            'traffic.speed_limit.120':
            'traffic_sign',
            'traffic.stop':
            'traffic_sign',
            'traffic.yield':
            'traffic_sign',
            'traffic.traffic_light':
            'traffic_light',
            'static.prop.warningconstruction':
            'traffic_cone',
            'static.prop.warningaccident':
            'traffic_cone',
            'static.prop.trafficwarning':
            'traffic_cone',
            'static.prop.constructioncone':
            'traffic_cone',
            'walker.pedestrian.0001':
            'pedestrian',
            'walker.pedestrian.0003':
            'pedestrian',
            'walker.pedestrian.0004':
            'pedestrian',
            'walker.pedestrian.0005':
            'pedestrian',
            'walker.pedestrian.0007':
            'pedestrian',
            'walker.pedestrian.0010':
            'pedestrian',
            'walker.pedestrian.0013':
            'pedestrian',
            'walker.pedestrian.0014':
            'pedestrian',
            'walker.pedestrian.0015':
            'pedestrian',
            'walker.pedestrian.0016':
            'pedestrian',
            'walker.pedestrian.0017':
            'pedestrian',
            'walker.pedestrian.0018':
            'pedestrian',
            'walker.pedestrian.0019':
            'pedestrian',
            'walker.pedestrian.0020':
            'pedestrian',
            'walker.pedestrian.0021':
            'pedestrian',
            'walker.pedestrian.0022':
            'pedestrian',
            'walker.pedestrian.0025':
            'pedestrian',
            'walker.pedestrian.0027':
            'pedestrian',
            'walker.pedestrian.0030':
            'pedestrian',
            'walker.pedestrian.0031':
            'pedestrian',
            'walker.pedestrian.0032':
            'pedestrian',
            'walker.pedestrian.0034':
            'pedestrian',
            'walker.pedestrian.0035':
            'pedestrian',
            'walker.pedestrian.0041':
            'pedestrian',
            'walker.pedestrian.0042':
            'pedestrian',
            'walker.pedestrian.0046':
            'pedestrian',
            'walker.pedestrian.0047':
            'pedestrian',
            'static.prop.dirtdebris01':
            'others',
            'static.prop.dirtdebris02':
            'others'
        }),
        map_root='data/bench2drive/maps',
        map_file='data/infos/b2d_map_infos.pkl',
        queue_length=1,
        past_frames=2,
        future_frames=6,
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        polyline_points_num=11),
    val=dict(
        type='B2DOrionDataset',
        data_root='data/bench2drive',
        ann_file='data/infos/b2d_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=True),
            dict(
                type='VADObjectRangeFilter',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='VADObjectNameFilter',
                classes=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ]),
            dict(
                type='ResizeCropFlipRotImage',
                data_aug_conf=dict(
                    resize_lim=(0.37, 0.45),
                    final_dim=(320, 640),
                    bot_pct_lim=(0.0, 0.0),
                    rot_lim=(0.0, 0.0),
                    H=900,
                    W=1600,
                    rand_flip=False),
                training=False),
            dict(
                type='ResizeMultiview3D',
                img_scale=(640, 640),
                keep_ratio=False,
                multiscale_mode='value'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnoatationCriticalVQATest',
                load_type=['critical_qa'],
                tokenizer='ckpts/pretrain_qformer/',
                use_gen_token=True,
                max_length=2048),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='PETRFormatBundle3D',
                        collect_keys=[
                            'lidar2img', 'cam_intrinsic', 'timestamp',
                            'ego_pose', 'ego_pose_inv', 'command'
                        ],
                        class_names=[
                            'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                            'traffic_cone', 'traffic_light', 'pedestrian',
                            'others'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'gt_bboxes_3d', 'gt_labels_3d', 'img',
                            'ego_his_trajs', 'input_ids', 'gt_attr_labels',
                            'ego_fut_trajs', 'ego_fut_masks', 'ego_fut_cmd',
                            'ego_lcf_feat', 'vlm_labels', 'can_bus',
                            'fut_valid_flag', 'lidar2img', 'cam_intrinsic',
                            'timestamp', 'ego_pose', 'ego_pose_inv', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        name_mapping=dict({
            'vehicle.bh.crossbike':
            'bicycle',
            'vehicle.diamondback.century':
            'bicycle',
            'vehicle.gazelle.omafiets':
            'bicycle',
            'vehicle.audi.etron':
            'car',
            'vehicle.chevrolet.impala':
            'car',
            'vehicle.dodge.charger_2020':
            'car',
            'vehicle.dodge.charger_police':
            'car',
            'vehicle.dodge.charger_police_2020':
            'car',
            'vehicle.lincoln.mkz_2017':
            'car',
            'vehicle.lincoln.mkz_2020':
            'car',
            'vehicle.mini.cooper_s_2021':
            'car',
            'vehicle.mercedes.coupe_2020':
            'car',
            'vehicle.ford.mustang':
            'car',
            'vehicle.nissan.patrol_2021':
            'car',
            'vehicle.audi.tt':
            'car',
            'vehicle.ford.crown':
            'car',
            'vehicle.tesla.model3':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/FordCrown/SM_FordCrown_parked.SM_FordCrown_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Charger/SM_ChargerParked.SM_ChargerParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Lincoln/SM_LincolnParked.SM_LincolnParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/MercedesCCC/SM_MercedesCCC_Parked.SM_MercedesCCC_Parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Mini2021/SM_Mini2021_parked.SM_Mini2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/NissanPatrol2021/SM_NissanPatrol2021_parked.SM_NissanPatrol2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/TeslaM3/SM_TeslaM3_parked.SM_TeslaM3_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/VolkswagenT2/SM_VolkswagenT2_2021_Parked.SM_VolkswagenT2_2021_Parked':
            'van',
            'vehicle.ford.ambulance':
            'van',
            'vehicle.carlamotors.firetruck':
            'truck',
            'traffic.speed_limit.30':
            'traffic_sign',
            'traffic.speed_limit.40':
            'traffic_sign',
            'traffic.speed_limit.50':
            'traffic_sign',
            'traffic.speed_limit.60':
            'traffic_sign',
            'traffic.speed_limit.90':
            'traffic_sign',
            'traffic.speed_limit.120':
            'traffic_sign',
            'traffic.stop':
            'traffic_sign',
            'traffic.yield':
            'traffic_sign',
            'traffic.traffic_light':
            'traffic_light',
            'static.prop.warningconstruction':
            'traffic_cone',
            'static.prop.warningaccident':
            'traffic_cone',
            'static.prop.trafficwarning':
            'traffic_cone',
            'static.prop.constructioncone':
            'traffic_cone',
            'walker.pedestrian.0001':
            'pedestrian',
            'walker.pedestrian.0003':
            'pedestrian',
            'walker.pedestrian.0004':
            'pedestrian',
            'walker.pedestrian.0005':
            'pedestrian',
            'walker.pedestrian.0007':
            'pedestrian',
            'walker.pedestrian.0010':
            'pedestrian',
            'walker.pedestrian.0013':
            'pedestrian',
            'walker.pedestrian.0014':
            'pedestrian',
            'walker.pedestrian.0015':
            'pedestrian',
            'walker.pedestrian.0016':
            'pedestrian',
            'walker.pedestrian.0017':
            'pedestrian',
            'walker.pedestrian.0018':
            'pedestrian',
            'walker.pedestrian.0019':
            'pedestrian',
            'walker.pedestrian.0020':
            'pedestrian',
            'walker.pedestrian.0021':
            'pedestrian',
            'walker.pedestrian.0022':
            'pedestrian',
            'walker.pedestrian.0025':
            'pedestrian',
            'walker.pedestrian.0027':
            'pedestrian',
            'walker.pedestrian.0030':
            'pedestrian',
            'walker.pedestrian.0031':
            'pedestrian',
            'walker.pedestrian.0032':
            'pedestrian',
            'walker.pedestrian.0034':
            'pedestrian',
            'walker.pedestrian.0035':
            'pedestrian',
            'walker.pedestrian.0041':
            'pedestrian',
            'walker.pedestrian.0042':
            'pedestrian',
            'walker.pedestrian.0046':
            'pedestrian',
            'walker.pedestrian.0047':
            'pedestrian',
            'static.prop.dirtdebris01':
            'others',
            'static.prop.dirtdebris02':
            'others'
        }),
        map_root='data/bench2drive/maps',
        map_file='data/infos/b2d_map_infos.pkl',
        queue_length=1,
        past_frames=2,
        future_frames=6,
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        polyline_points_num=11,
        eval_cfg=dict(
            dist_ths=[0.5, 1.0, 2.0, 4.0],
            dist_th_tp=2.0,
            min_recall=0.1,
            min_precision=0.1,
            mean_ap_weight=5,
            class_names=[
                'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                'traffic_cone', 'traffic_light', 'pedestrian'
            ],
            tp_metrics=['trans_err', 'scale_err', 'orient_err', 'vel_err'],
            err_name_maping=dict(
                trans_err='mATE',
                scale_err='mASE',
                orient_err='mAOE',
                vel_err='mAVE',
                attr_err='mAAE'),
            class_range=dict(
                car=(50, 50),
                van=(50, 50),
                truck=(50, 50),
                bicycle=(40, 40),
                traffic_sign=(30, 30),
                traffic_cone=(30, 30),
                traffic_light=(30, 30),
                pedestrian=(40, 40)))),
    test=dict(
        type='B2DOrionDataset',
        data_root='data/bench2drive',
        ann_file='data/infos/b2d_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=True),
            dict(
                type='VADObjectRangeFilter',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='VADObjectNameFilter',
                classes=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ]),
            dict(
                type='ResizeCropFlipRotImage',
                data_aug_conf=dict(
                    resize_lim=(0.37, 0.45),
                    final_dim=(320, 640),
                    bot_pct_lim=(0.0, 0.0),
                    rot_lim=(0.0, 0.0),
                    H=900,
                    W=1600,
                    rand_flip=False),
                training=False),
            dict(
                type='ResizeMultiview3D',
                img_scale=(640, 640),
                keep_ratio=False,
                multiscale_mode='value'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnoatationCriticalVQATest',
                load_type=['critical_qa'],
                tokenizer='ckpts/pretrain_qformer/',
                use_gen_token=True,
                max_length=2048),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='PETRFormatBundle3D',
                        collect_keys=[
                            'lidar2img', 'cam_intrinsic', 'timestamp',
                            'ego_pose', 'ego_pose_inv', 'command'
                        ],
                        class_names=[
                            'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                            'traffic_cone', 'traffic_light', 'pedestrian',
                            'others'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'gt_bboxes_3d', 'gt_labels_3d', 'img',
                            'ego_his_trajs', 'input_ids', 'gt_attr_labels',
                            'ego_fut_trajs', 'ego_fut_masks', 'ego_fut_cmd',
                            'ego_lcf_feat', 'vlm_labels', 'can_bus',
                            'fut_valid_flag', 'lidar2img', 'cam_intrinsic',
                            'timestamp', 'ego_pose', 'ego_pose_inv', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        name_mapping=dict({
            'vehicle.bh.crossbike':
            'bicycle',
            'vehicle.diamondback.century':
            'bicycle',
            'vehicle.gazelle.omafiets':
            'bicycle',
            'vehicle.audi.etron':
            'car',
            'vehicle.chevrolet.impala':
            'car',
            'vehicle.dodge.charger_2020':
            'car',
            'vehicle.dodge.charger_police':
            'car',
            'vehicle.dodge.charger_police_2020':
            'car',
            'vehicle.lincoln.mkz_2017':
            'car',
            'vehicle.lincoln.mkz_2020':
            'car',
            'vehicle.mini.cooper_s_2021':
            'car',
            'vehicle.mercedes.coupe_2020':
            'car',
            'vehicle.ford.mustang':
            'car',
            'vehicle.nissan.patrol_2021':
            'car',
            'vehicle.audi.tt':
            'car',
            'vehicle.ford.crown':
            'car',
            'vehicle.tesla.model3':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/FordCrown/SM_FordCrown_parked.SM_FordCrown_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Charger/SM_ChargerParked.SM_ChargerParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Lincoln/SM_LincolnParked.SM_LincolnParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/MercedesCCC/SM_MercedesCCC_Parked.SM_MercedesCCC_Parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Mini2021/SM_Mini2021_parked.SM_Mini2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/NissanPatrol2021/SM_NissanPatrol2021_parked.SM_NissanPatrol2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/TeslaM3/SM_TeslaM3_parked.SM_TeslaM3_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/VolkswagenT2/SM_VolkswagenT2_2021_Parked.SM_VolkswagenT2_2021_Parked':
            'van',
            'vehicle.ford.ambulance':
            'van',
            'vehicle.carlamotors.firetruck':
            'truck',
            'traffic.speed_limit.30':
            'traffic_sign',
            'traffic.speed_limit.40':
            'traffic_sign',
            'traffic.speed_limit.50':
            'traffic_sign',
            'traffic.speed_limit.60':
            'traffic_sign',
            'traffic.speed_limit.90':
            'traffic_sign',
            'traffic.speed_limit.120':
            'traffic_sign',
            'traffic.stop':
            'traffic_sign',
            'traffic.yield':
            'traffic_sign',
            'traffic.traffic_light':
            'traffic_light',
            'static.prop.warningconstruction':
            'traffic_cone',
            'static.prop.warningaccident':
            'traffic_cone',
            'static.prop.trafficwarning':
            'traffic_cone',
            'static.prop.constructioncone':
            'traffic_cone',
            'walker.pedestrian.0001':
            'pedestrian',
            'walker.pedestrian.0003':
            'pedestrian',
            'walker.pedestrian.0004':
            'pedestrian',
            'walker.pedestrian.0005':
            'pedestrian',
            'walker.pedestrian.0007':
            'pedestrian',
            'walker.pedestrian.0010':
            'pedestrian',
            'walker.pedestrian.0013':
            'pedestrian',
            'walker.pedestrian.0014':
            'pedestrian',
            'walker.pedestrian.0015':
            'pedestrian',
            'walker.pedestrian.0016':
            'pedestrian',
            'walker.pedestrian.0017':
            'pedestrian',
            'walker.pedestrian.0018':
            'pedestrian',
            'walker.pedestrian.0019':
            'pedestrian',
            'walker.pedestrian.0020':
            'pedestrian',
            'walker.pedestrian.0021':
            'pedestrian',
            'walker.pedestrian.0022':
            'pedestrian',
            'walker.pedestrian.0025':
            'pedestrian',
            'walker.pedestrian.0027':
            'pedestrian',
            'walker.pedestrian.0030':
            'pedestrian',
            'walker.pedestrian.0031':
            'pedestrian',
            'walker.pedestrian.0032':
            'pedestrian',
            'walker.pedestrian.0034':
            'pedestrian',
            'walker.pedestrian.0035':
            'pedestrian',
            'walker.pedestrian.0041':
            'pedestrian',
            'walker.pedestrian.0042':
            'pedestrian',
            'walker.pedestrian.0046':
            'pedestrian',
            'walker.pedestrian.0047':
            'pedestrian',
            'static.prop.dirtdebris01':
            'others',
            'static.prop.dirtdebris02':
            'others'
        }),
        map_root='data/bench2drive/maps',
        map_file='data/infos/b2d_map_infos.pkl',
        queue_length=1,
        past_frames=2,
        future_frames=6,
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        polyline_points_num=11,
        eval_cfg=dict(
            dist_ths=[0.5, 1.0, 2.0, 4.0],
            dist_th_tp=2.0,
            min_recall=0.1,
            min_precision=0.1,
            mean_ap_weight=5,
            class_names=[
                'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                'traffic_cone', 'traffic_light', 'pedestrian'
            ],
            tp_metrics=['trans_err', 'scale_err', 'orient_err', 'vel_err'],
            err_name_maping=dict(
                trans_err='mATE',
                scale_err='mASE',
                orient_err='mAOE',
                vel_err='mAVE',
                attr_err='mAAE'),
            class_range=dict(
                car=(50, 50),
                van=(50, 50),
                truck=(50, 50),
                bicycle=(40, 40),
                traffic_sign=(30, 30),
                traffic_cone=(30, 30),
                traffic_light=(30, 30),
                pedestrian=(40, 40)))),
    shuffler_sampler=dict(
        type='InfiniteGroupEachSampleInBatchSampler',
        seq_split_num=10,
        warmup_split_num=80,
        num_iters_to_seq=1834),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=12838,
    pipeline=[
        dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
        dict(
            type='LoadAnnotations3D',
            with_bbox_3d=True,
            with_label_3d=True,
            with_attr_label=True),
        dict(
            type='VADObjectRangeFilter',
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        dict(
            type='VADObjectNameFilter',
            classes=[
                'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                'traffic_cone', 'traffic_light', 'pedestrian', 'others'
            ]),
        dict(
            type='ResizeCropFlipRotImage',
            data_aug_conf=dict(
                resize_lim=(0.37, 0.45),
                final_dim=(320, 640),
                bot_pct_lim=(0.0, 0.0),
                rot_lim=(0.0, 0.0),
                H=900,
                W=1600,
                rand_flip=False),
            training=False),
        dict(
            type='ResizeMultiview3D',
            img_scale=(640, 640),
            keep_ratio=False,
            multiscale_mode='value'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[123.675, 116.28, 103.53],
            std=[58.395, 57.12, 57.375],
            to_rgb=True),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnoatationCriticalVQATest',
            load_type=['critical_qa'],
            tokenizer='ckpts/pretrain_qformer/',
            use_gen_token=True,
            max_length=2048),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1333, 800),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='PETRFormatBundle3D',
                    collect_keys=[
                        'lidar2img', 'cam_intrinsic', 'timestamp', 'ego_pose',
                        'ego_pose_inv', 'command'
                    ],
                    class_names=[
                        'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                        'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'gt_bboxes_3d', 'gt_labels_3d', 'img', 'ego_his_trajs',
                        'input_ids', 'gt_attr_labels', 'ego_fut_trajs',
                        'ego_fut_masks', 'ego_fut_cmd', 'ego_lcf_feat',
                        'vlm_labels', 'can_bus', 'fut_valid_flag', 'lidar2img',
                        'cam_intrinsic', 'timestamp', 'ego_pose',
                        'ego_pose_inv', 'command'
                    ])
            ])
    ])
checkpoint_config = dict(interval=1834, max_keep_ckpts=3)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = 'adzoo/orion/work_dirs/orion_stage2_train/'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
backbone_norm_cfg = dict(type='LN', requires_grad=True)
voxel_size = [0.2, 0.2, 8]
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
map_classes = [
    'Broken', 'Solid', 'SolidSolid', 'Center', 'TrafficLight', 'StopSign'
]
queue_length = 1
map_fixed_ptsnum_per_gt_line = 11
map_eval_use_same_gt_sample_num_flag = True
map_num_classes = 6
past_frames = 2
future_frames = 6
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
ida_aug_conf = dict(
    resize_lim=(0.37, 0.45),
    final_dim=(320, 640),
    bot_pct_lim=(0.0, 0.0),
    rot_lim=(0.0, 0.0),
    H=900,
    W=1600,
    rand_flip=False)
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
NameMapping = dict({
    'vehicle.bh.crossbike':
    'bicycle',
    'vehicle.diamondback.century':
    'bicycle',
    'vehicle.gazelle.omafiets':
    'bicycle',
    'vehicle.audi.etron':
    'car',
    'vehicle.chevrolet.impala':
    'car',
    'vehicle.dodge.charger_2020':
    'car',
    'vehicle.dodge.charger_police':
    'car',
    'vehicle.dodge.charger_police_2020':
    'car',
    'vehicle.lincoln.mkz_2017':
    'car',
    'vehicle.lincoln.mkz_2020':
    'car',
    'vehicle.mini.cooper_s_2021':
    'car',
    'vehicle.mercedes.coupe_2020':
    'car',
    'vehicle.ford.mustang':
    'car',
    'vehicle.nissan.patrol_2021':
    'car',
    'vehicle.audi.tt':
    'car',
    'vehicle.ford.crown':
    'car',
    'vehicle.tesla.model3':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/FordCrown/SM_FordCrown_parked.SM_FordCrown_parked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Charger/SM_ChargerParked.SM_ChargerParked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Lincoln/SM_LincolnParked.SM_LincolnParked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/MercedesCCC/SM_MercedesCCC_Parked.SM_MercedesCCC_Parked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Mini2021/SM_Mini2021_parked.SM_Mini2021_parked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/NissanPatrol2021/SM_NissanPatrol2021_parked.SM_NissanPatrol2021_parked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/TeslaM3/SM_TeslaM3_parked.SM_TeslaM3_parked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/VolkswagenT2/SM_VolkswagenT2_2021_Parked.SM_VolkswagenT2_2021_Parked':
    'van',
    'vehicle.ford.ambulance':
    'van',
    'vehicle.carlamotors.firetruck':
    'truck',
    'traffic.speed_limit.30':
    'traffic_sign',
    'traffic.speed_limit.40':
    'traffic_sign',
    'traffic.speed_limit.50':
    'traffic_sign',
    'traffic.speed_limit.60':
    'traffic_sign',
    'traffic.speed_limit.90':
    'traffic_sign',
    'traffic.speed_limit.120':
    'traffic_sign',
    'traffic.stop':
    'traffic_sign',
    'traffic.yield':
    'traffic_sign',
    'traffic.traffic_light':
    'traffic_light',
    'static.prop.warningconstruction':
    'traffic_cone',
    'static.prop.warningaccident':
    'traffic_cone',
    'static.prop.trafficwarning':
    'traffic_cone',
    'static.prop.constructioncone':
    'traffic_cone',
    'walker.pedestrian.0001':
    'pedestrian',
    'walker.pedestrian.0003':
    'pedestrian',
    'walker.pedestrian.0004':
    'pedestrian',
    'walker.pedestrian.0005':
    'pedestrian',
    'walker.pedestrian.0007':
    'pedestrian',
    'walker.pedestrian.0010':
    'pedestrian',
    'walker.pedestrian.0013':
    'pedestrian',
    'walker.pedestrian.0014':
    'pedestrian',
    'walker.pedestrian.0015':
    'pedestrian',
    'walker.pedestrian.0016':
    'pedestrian',
    'walker.pedestrian.0017':
    'pedestrian',
    'walker.pedestrian.0018':
    'pedestrian',
    'walker.pedestrian.0019':
    'pedestrian',
    'walker.pedestrian.0020':
    'pedestrian',
    'walker.pedestrian.0021':
    'pedestrian',
    'walker.pedestrian.0022':
    'pedestrian',
    'walker.pedestrian.0025':
    'pedestrian',
    'walker.pedestrian.0027':
    'pedestrian',
    'walker.pedestrian.0030':
    'pedestrian',
    'walker.pedestrian.0031':
    'pedestrian',
    'walker.pedestrian.0032':
    'pedestrian',
    'walker.pedestrian.0034':
    'pedestrian',
    'walker.pedestrian.0035':
    'pedestrian',
    'walker.pedestrian.0041':
    'pedestrian',
    'walker.pedestrian.0042':
    'pedestrian',
    'walker.pedestrian.0046':
    'pedestrian',
    'walker.pedestrian.0047':
    'pedestrian',
    'static.prop.dirtdebris01':
    'others',
    'static.prop.dirtdebris02':
    'others'
})
eval_cfg = dict(
    dist_ths=[0.5, 1.0, 2.0, 4.0],
    dist_th_tp=2.0,
    min_recall=0.1,
    min_precision=0.1,
    mean_ap_weight=5,
    class_names=[
        'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
        'traffic_light', 'pedestrian'
    ],
    tp_metrics=['trans_err', 'scale_err', 'orient_err', 'vel_err'],
    err_name_maping=dict(
        trans_err='mATE',
        scale_err='mASE',
        orient_err='mAOE',
        vel_err='mAVE',
        attr_err='mAAE'),
    class_range=dict(
        car=(50, 50),
        van=(50, 50),
        truck=(50, 50),
        bicycle=(40, 40),
        traffic_sign=(30, 30),
        traffic_cone=(30, 30),
        traffic_light=(30, 30),
        pedestrian=(40, 40)))
use_memory = True
num_gpus = 32
batch_size = 4
num_iters_per_epoch = 1834
num_epochs = 6
llm_path = 'ckpts/pretrain_qformer/'
use_gen_token = True
use_col_loss = True
collect_keys = [
    'lidar2img', 'cam_intrinsic', 'timestamp', 'ego_pose', 'ego_pose_inv',
    'command'
]
model = dict(
    type='Orion',
    save_path='./results_planning_only/',
    use_grid_mask=True,
    frozen=False,
    use_lora=True,
    tokenizer='ckpts/pretrain_qformer/',
    lm_head='ckpts/pretrain_qformer/',
    use_gen_token=True,
    use_diff_decoder=False,
    use_col_loss=True,
    loss_plan_reg=dict(type='L1Loss', loss_weight=3.0),
    loss_plan_bound=dict(
        type='PlanMapBoundLoss', loss_weight=3.0, dis_thresh=1.0),
    loss_plan_col=dict(type='PlanCollisionLoss', loss_weight=1.0),
    loss_vae_gen=dict(type='ProbabilisticLoss', loss_weight=3.0),
    img_backbone=dict(
        type='EVAViT',
        img_size=640,
        patch_size=16,
        window_size=16,
        in_chans=3,
        embed_dim=1024,
        depth=24,
        num_heads=16,
        mlp_ratio=2.6666666666666665,
        window_block_indexes=[
            0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 22
        ],
        qkv_bias=True,
        drop_path_rate=0.3,
        flash_attn=True,
        with_cp=True,
        frozen=False),
    map_head=dict(
        type='OrionHeadM',
        num_classes=6,
        in_channels=1024,
        out_dims=4096,
        memory_len=600,
        with_mask=True,
        topk_proposals=300,
        num_lane=1800,
        num_lanes_one2one=300,
        k_one2many=5,
        lambda_one2many=1.0,
        num_extra=256,
        n_control=11,
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        code_weights=[1.0, 1.0],
        score_threshold=0.2,
        transformer=dict(
            type='PETRTemporalTransformer',
            input_dimension=256,
            output_dimension=256,
            num_layers=6,
            embed_dims=256,
            num_heads=8,
            feedforward_dims=2048,
            dropout=0.1,
            with_cp=True,
            flash_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='LaneHungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=1.5),
                reg_cost=dict(type='LaneL1Cost', weight=0.02),
                iou_cost=dict(type='IoUCost', weight=0.0))),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.5),
        loss_bbox=dict(type='L1Loss', loss_weight=0.02),
        loss_dir=dict(type='PtsDirCosLoss', loss_weight=0.0)),
    pts_bbox_head=dict(
        type='OrionHead',
        num_classes=9,
        in_channels=1024,
        out_dims=4096,
        num_query=600,
        with_mask=True,
        memory_len=600,
        topk_proposals=300,
        num_propagated=300,
        num_extra=256,
        n_control=11,
        match_with_velo=False,
        pred_traffic_light_state=True,
        use_col_loss=True,
        use_memory=True,
        scalar=10,
        noise_scale=1.0,
        dn_weight=1.0,
        split=0.75,
        use_pe=False,
        motion_transformer_decoder=dict(
            type='OrionTransformerDecoder',
            num_layers=1,
            embed_dims=256,
            num_heads=8,
            dropout=0.0,
            feedforward_dims=512,
            with_cp=True,
            flash_attn=True,
            return_intermediate=False),
        code_weights=[2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
        score_threshold=0.2,
        class_agnostic_nms=dict(
            classes=[0, 1, 2, 3, 4, 5, 6, 7, 8],
            compensate=[0, 0, 0.3, 0, 0, 0, 0, 0.3, 0],
            pre_max_size=1000,
            post_max_size=300,
            nms_thr=0.1),
        memory_decoder_transformer=dict(
            type='OrionTransformerDecoder',
            num_layers=1,
            embed_dims=256,
            num_heads=8,
            dropout=0.0,
            feedforward_dims=512,
            with_cp=True,
            flash_attn=True,
            return_intermediate=False),
        transformer=dict(
            type='PETRTemporalTransformer',
            input_dimension=256,
            output_dimension=256,
            num_layers=6,
            embed_dims=256,
            num_heads=8,
            feedforward_dims=2048,
            dropout=0.1,
            with_cp=True,
            flash_attn=True),
        bbox_coder=dict(
            type='CustomNMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=9),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_traffic=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos'
map_root = 'data/bench2drive/maps'
map_file = 'data/infos/b2d_map_infos.pkl'
ann_file_train = 'data/infos/b2d_infos_train.pkl'
ann_file_val = 'data/infos/b2d_infos_val.pkl'
ann_file_test = 'data/infos/b2d_infos_val.pkl'
inference_only_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/bench2drive'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=['img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'command'])
        ])
]
optimizer = dict(
    constructor='LearningRateDecayOptimizerConstructor',
    type='AdamW',
    lr=8e-05,
    betas=(0.9, 0.999),
    weight_decay=1e-05,
    paramwise_cfg=dict(
        decay_rate=0.9,
        head_decay_rate=4.0,
        lm_head_decay_rate=0.1,
        decay_type='vit_wise',
        num_layers=24))
optimizer_config = dict(
    type='Fp16OptimizerHook',
    loss_scale='dynamic',
    grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
find_unused_parameters = False
runner = dict(type='IterBasedRunner', max_iters=11004)
gpu_ids = range(0, 1)

2026-01-14 04:35:09,667 - mmdet - INFO - Set random seed to 0, deterministic: True
======== shape of rope freq torch.Size([256, 64]) ========
======== shape of rope freq torch.Size([1600, 64]) ========
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
Loading checkpoint shards:  50%|     | 1/2 [00:41<00:41, 41.41s/it]Loading checkpoint shards: 100%|| 2/2 [01:01<00:00, 28.89s/it]Loading checkpoint shards: 100%|| 2/2 [01:01<00:00, 30.77s/it]
Some weights of the model checkpoint at ckpts/pretrain_qformer/ were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.blocks.0.mlp.w1.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.0.attn.out_proj.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.blocks.12.mlp.w1.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.8.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.12.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.21.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.14.attn.v_bias', 'model.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.v_proj.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.blocks.12.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.w2.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.4._layers.0.weight', 'model.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.blocks.23.attn.v_bias', 'model.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.0.attn.out_proj.bias', 'model.mm_projector.query_decoder._layers.0.transformer_layers.5.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.0.attn.in_proj_weight', 'model.vision_tower.vision_tower.blocks.18.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.12.attn.proj.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.1.bias', 'model.vision_tower.vision_tower.blocks.14.attn.q_bias', 'model.vision_tower.vision_tower.blocks.17.norm1.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.0.attn.out_proj.weight', 'model.vision_tower.vision_tower.blocks.12.norm2.bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.13.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.blocks.14.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.3.attn.q_proj.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.2.attn.in_proj_weight', 'model.vision_tower.vision_tower.blocks.16.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.w3.bias', 'model.vision_tower.vision_tower.rope_glb.freqs_cos', 'model.mm_projector.query_decoder._layers.2.transformer_layers.1.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.bias', 'model.vision_tower.vision_tower.blocks.21.norm1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.mlp.w1.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.4._layers.0.bias', 'model.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vision_tower.vision_tower.blocks.1.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.17.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.23.attn.rope.freqs_sin', 'model.mm_projector.query_decoder._layers.5.transformer_layers.0.attn.in_proj_weight', 'model.vision_tower.vision_tower.blocks.13.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.12.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.14.norm1.bias', 'model.vision_tower.vision_tower.blocks.20.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.23.attn.q_bias', 'model.vision_tower.vision_tower.blocks.1.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.w2.bias', 'model.mm_projector.query_decoder._layers.0.transformer_layers.2.attn.in_proj_weight', 'model.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.weight', 'model.vision_tower.vision_tower.blocks.22.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.3.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.20.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.7.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.8.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.w2.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.1.bias', 'model.mm_projector.input_projection.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.blocks.21.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.2.attn.in_proj_bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.4._layers.3.weight', 'model.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.vision_tower.vision_tower.rope_win.freqs_cos', 'model.vision_tower.vision_tower.blocks.16.attn.rope.freqs_sin', 'model.mm_projector.query_decoder._layers.2.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.13.attn.q_bias', 'model.vision_tower.vision_tower.blocks.10.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.4._layers.3.weight', 'model.vision_tower.vision_tower.blocks.13.norm1.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.15.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.w2.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.k_proj.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.4._layers.0.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.blocks.12.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.5.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.19.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.15.attn.q_proj.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.3.weight', 'model.vision_tower.vision_tower.blocks.1.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vision_tower.vision_tower.blocks.13.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.11.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.1.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.12.attn.v_bias', 'model.vision_tower.vision_tower.blocks.21.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.1.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.20.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.18.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.16.attn.rope.freqs_cos', 'model.mm_projector.query_decoder._layers.1.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.blocks.1.mlp.w2.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.blocks.15.norm1.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.2.attn.out_proj.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.4._layers.0.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.1.weight', 'model.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.blocks.6.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.17.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.17.norm2.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.w1.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.5.bias', 'model.vision_tower.vision_tower.blocks.8.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.14.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.vision_tower.vision_tower.blocks.10.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.13.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.2.attn.out_proj.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.0.attn.out_proj.weight', 'model.vision_tower.vision_tower.blocks.20.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.10.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.9.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.12.attn.q_bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.1.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.2.attn.in_proj_bias', 'model.vision_tower.vision_tower.blocks.10.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.21.attn.v_bias', 'model.vision_tower.vision_tower.blocks.3.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.12.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.16.norm1.weight', 'model.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.blocks.19.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.16.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.15.norm2.bias', 'model.mm_projector.query_embedding.weight', 'model.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.w3.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.2.attn.in_proj_bias', 'model.vision_tower.vision_tower.blocks.18.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.2.attn.in_proj_weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.4._layers.0.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.0.attn.out_proj.bias', 'model.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.21.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.13.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.3.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.w1.weight', 'model.mm_projector.output_projection.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.vision_tower.vision_tower.rope_glb.freqs_sin', 'model.vision_tower.vision_tower.blocks.9.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.18.attn.v_bias', 'model.vision_tower.vision_tower.blocks.13.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.21.attn.q_bias', 'model.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.18.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.14.norm2.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_bias', 'model.vision_tower.vision_tower.blocks.7.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.blocks.1.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.1.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.w3.weight', 'model.mm_projector.output_projection.weight', 'model.vision_tower.vision_tower.blocks.14.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.3.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.w3.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.blocks.19.norm2.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.5.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.0.attn.out_proj.bias', 'model.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.5.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.4._layers.3.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.4._layers.0.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.blocks.11.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.19.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.23.norm1.weight', 'model.vision_tower.vision_tower.blocks.4.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.blocks.10.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.4._layers.0.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.2.attn.in_proj_bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.4._layers.0.bias', 'model.vision_tower.vision_tower.blocks.5.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.17.attn.v_bias', 'model.vision_tower.vision_tower.blocks.17.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.bias', 'model.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.attn.v_bias', 'model.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.2.attn.in_proj_bias', 'model.mm_projector.query_decoder._layers.0.transformer_layers.3.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.w1.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.blocks.15.attn.proj.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.1.weight', 'model.vision_tower.vision_tower.blocks.13.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.ffn_ln.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.2.attn.out_proj.weight', 'model.vision_tower.vision_tower.blocks.10.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.13.attn.v_bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.3.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.w3.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.blocks.3.attn.rope.freqs_cos', 'model.mm_projector.query_decoder._layers.3.transformer_layers.0.attn.out_proj.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.5.bias', 'model.vision_tower.vision_tower.blocks.5.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.8.attn.v_proj.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.2.attn.in_proj_weight', 'model.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.blocks.8.mlp.w2.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.5.weight', 'model.vision_tower.vision_tower.blocks.19.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.23.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.0.attn.in_proj_weight', 'model.vision_tower.vision_tower.blocks.21.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.20.attn.v_bias', 'model.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.8.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.12.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.blocks.9.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.w1.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.1.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.4._layers.3.weight', 'model.vision_tower.vision_tower.blocks.7.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.4.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vision_tower.vision_tower.blocks.17.norm2.weight', 'model.vision_tower.vision_tower.blocks.21.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.20.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.17.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.15.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.12.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.bias', 'model.vision_tower.vision_tower.blocks.7.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.2.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.23.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.12.norm1.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.ffn_ln.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.blocks.19.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.16.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.5.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.0.attn.in_proj_weight', 'model.vision_tower.vision_tower.blocks.9.attn.k_proj.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.5.bias', 'model.vision_tower.vision_tower.blocks.9.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.5.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.0.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.13.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.20.norm1.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.3.bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_bias', 'model.vision_tower.vision_tower.blocks.13.norm2.weight', 'model.vision_tower.vision_tower.blocks.16.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.11.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.14.norm1.weight', 'model.vision_tower.vision_tower.blocks.19.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_proj.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.5.weight', 'model.vision_tower.vision_tower.blocks.12.attn.k_proj.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.0.attn.out_proj.bias', 'model.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.vision_tower.vision_tower.blocks.3.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.4.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.17.attn.q_bias', 'model.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.blocks.10.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.21.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.16.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.17.norm1.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.ffn_ln.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.3.weight', 'model.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.5.weight', 'model.vision_tower.vision_tower.blocks.15.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.19.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.13.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.18.mlp.w3.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.2.attn.in_proj_weight', 'model.vision_tower.vision_tower.blocks.1.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.10.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.vision_tower.vision_tower.blocks.0.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vision_tower.vision_tower.blocks.3.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.17.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.6.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.20.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vision_tower.vision_tower.blocks.6.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.17.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.5.bias', 'model.vision_tower.vision_tower.blocks.21.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.vision_tower.vision_tower.blocks.17.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.9.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.15.norm1.bias', 'model.vision_tower.vision_tower.blocks.15.attn.v_proj.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.0.attn.out_proj.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.23.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.0.attn.k_proj.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.4._layers.3.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.1.bias', 'model.vision_tower.vision_tower.blocks.17.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.14.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.18.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.blocks.15.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.18.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.6.attn.rope.freqs_sin', 'model.mm_projector.query_decoder._layers.4.transformer_layers.3.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.blocks.6.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.blocks.7.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.7.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.22.attn.q_bias', 'model.vision_tower.vision_tower.blocks.12.mlp.w3.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.0.attn.out_proj.weight', 'model.vision_tower.vision_tower.blocks.15.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.22.norm1.bias', 'model.vision_tower.vision_tower.blocks.3.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.9.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.2.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.19.attn.v_bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.blocks.12.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.22.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.22.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.4.attn.k_proj.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.4._layers.3.weight', 'model.vision_tower.vision_tower.blocks.16.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.22.norm2.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.w1.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.2.attn.out_proj.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.5.bias', 'model.vision_tower.vision_tower.blocks.22.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.4.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.3.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.2.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.3.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.11.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.9.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.19.mlp.w3.bias', 'model.mm_projector.query_decoder._layers.0.transformer_layers.4._layers.0.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.2.attn.out_proj.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.4.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.14.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vision_tower.vision_tower.blocks.15.attn.rope.freqs_sin', 'model.mm_projector.query_decoder._layers.4.transformer_layers.3.bias', 'model.vision_tower.vision_tower.blocks.14.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.18.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.23.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.blocks.13.attn.v_proj.weight', 'model.vision_tower.vision_tower.blocks.15.attn.q_bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.blocks.18.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.18.norm1.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.2.attn.in_proj_weight', 'model.vision_tower.vision_tower.blocks.3.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.13.norm2.bias', 'model.vision_tower.vision_tower.blocks.2.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.16.attn.q_bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.vision_tower.vision_tower.blocks.15.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.2.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.7.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.14.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.w1.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.4._layers.0.bias', 'model.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.0.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.15.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.11.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.22.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.23.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.16.mlp.w2.weight', 'model.vision_tower.vision_tower.blocks.13.attn.q_proj.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.0.attn.in_proj_weight', 'model.vision_tower.vision_tower.blocks.20.norm2.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.1.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.0.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.vision_tower.vision_tower.blocks.7.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.22.mlp.w2.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.0.attn.out_proj.bias', 'model.vision_tower.vision_tower.blocks.18.norm2.weight', 'model.vision_tower.vision_tower.blocks.8.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.11.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.2.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.22.attn.v_bias', 'model.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.blocks.18.norm1.bias', 'model.vision_tower.vision_tower.blocks.6.mlp.w1.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.3.bias', 'model.vision_tower.vision_tower.blocks.16.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.21.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.blocks.1.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.0.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.blocks.23.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.14.norm2.bias', 'model.vision_tower.vision_tower.blocks.20.attn.proj.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.3.weight', 'model.vision_tower.vision_tower.blocks.23.mlp.w1.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.2.attn.out_proj.weight', 'model.vision_tower.vision_tower.blocks.14.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.19.norm2.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.5.bias', 'model.vision_tower.vision_tower.rope_win.freqs_sin', 'model.vision_tower.vision_tower.blocks.20.mlp.w1.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.1.bias', 'model.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.2.attn.in_proj_bias', 'model.vision_tower.vision_tower.patch_embed.proj.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.blocks.15.attn.v_bias', 'model.vision_tower.vision_tower.blocks.19.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.12.norm2.weight', 'model.vision_tower.vision_tower.blocks.20.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.21.norm2.bias', 'model.vision_tower.vision_tower.blocks.16.norm2.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.w3.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.3.weight', 'model.vision_tower.vision_tower.blocks.17.attn.proj.weight', 'model.vision_tower.vision_tower.blocks.22.attn.proj.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.4._layers.0.weight', 'model.vision_tower.vision_tower.blocks.22.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.13.mlp.w3.bias', 'model.vision_tower.vision_tower.blocks.20.norm1.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.4._layers.0.bias', 'model.vision_tower.vision_tower.blocks.18.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.15.mlp.w3.weight', 'model.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.5.weight', 'model.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.blocks.23.norm2.weight', 'model.vision_tower.vision_tower.blocks.12.mlp.w2.bias', 'model.vision_tower.vision_tower.blocks.19.mlp.w1.bias', 'model.vision_tower.vision_tower.blocks.23.attn.q_proj.weight', 'model.vision_tower.vision_tower.blocks.4.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.blocks.0.mlp.w2.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.3.weight', 'model.vision_tower.vision_tower.blocks.5.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vision_tower.vision_tower.blocks.4.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.14.mlp.w1.weight', 'model.vision_tower.vision_tower.blocks.20.norm2.weight', 'model.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.vision_tower.vision_tower.blocks.16.norm1.bias', 'model.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.vision_tower.vision_tower.blocks.6.attn.k_proj.weight', 'model.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vision_tower.vision_tower.blocks.11.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.blocks.14.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.vision_tower.vision_tower.blocks.17.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.0.attn.in_proj_weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.0.attn.out_proj.weight', 'model.mm_projector.input_projection.bias', 'model.vision_tower.vision_tower.blocks.17.attn.v_proj.weight']
- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at ckpts/pretrain_qformer/ and are newly initialized: ['model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'weighted_mask']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2026-01-14 04:36:28,966 - mmdet - INFO - Model:
Orion(
  (pts_bbox_head): OrionHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (traj_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=12, bias=True)
      )
    )
    (traj_cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=512, out_features=1, bias=True)
      )
    )
    (cls_branches): ModuleList(
      (0-5): 6 x Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=9, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0-5): 6 x Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (input_projection): Linear(in_features=1024, out_features=256, bias=True)
    (output_projection): Linear(in_features=256, out_features=4096, bias=True)
    (reference_points): Embedding(600, 3)
    (pseudo_reference_points): Embedding(300, 3)
    (query_embedding): Embedding(256, 256)
    (can_bus_embed): Sequential(
      (0): Linear(in_features=89, out_features=1024, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1024, out_features=4096, bias=True)
    )
    (loss_traj): L1Loss()
    (loss_traj_cls): FocalLoss()
    (loss_iou): GIoULoss()
    (transformer): PETRTemporalTransformer(
      (query_decoder): PETRTransformerDecoder(
        (_layers): ModuleList(
          (0-5): 6 x PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
    )
    (motion_decoder): OrionTransformerDecoder(
      (_layers): ModuleList(
        (0): OrionTransformerDecoderLayer(
          (transformer_layers): ModuleList(
            (0): MultiHeadAttentionwDropout(
              (attn): FlashMHA(
                (inner_attn): FlashAttention()
                (out_proj): Linear(in_features=256, out_features=256, bias=False)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): FFN(
              (_layers): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=512, out_features=256, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (motion_mode_query): Embedding(6, 256)
    (memory_query): Embedding(16, 256)
    (scene_time_embedding): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (memory_decoder_cq): OrionTransformerDecoder(
      (_layers): ModuleList(
        (0): OrionTransformerDecoderLayer(
          (transformer_layers): ModuleList(
            (0): MultiHeadAttentionwDropout(
              (attn): FlashMHA(
                (inner_attn): FlashAttention()
                (out_proj): Linear(in_features=256, out_features=256, bias=False)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): FFN(
              (_layers): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=512, out_features=256, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (memory_decoder_mq): OrionTransformerDecoder(
      (_layers): ModuleList(
        (0): OrionTransformerDecoderLayer(
          (transformer_layers): ModuleList(
            (0): MultiHeadAttentionwDropout(
              (attn): FlashMHA(
                (inner_attn): FlashAttention()
                (out_proj): Linear(in_features=256, out_features=256, bias=False)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): FFN(
              (_layers): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=512, out_features=256, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (loss_traffic): FocalLoss()
    (tl_branches): ModuleList(
      (0-5): 6 x Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_pos): Sequential(
      (0): Linear(in_features=396, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (time_embedding): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (ego_pose_pe): MLN(
      (reduce): Sequential(
        (0): Linear(in_features=156, out_features=256, bias=True)
        (1): ReLU()
      )
      (gamma): Linear(in_features=256, out_features=256, bias=True)
      (beta): Linear(in_features=256, out_features=256, bias=True)
      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
    )
  )
  (img_backbone): EVAViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
    )
    (rope_win): VisionRotaryEmbeddingFast()
    (rope_glb): VisionRotaryEmbeddingFast()
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (rope): VisionRotaryEmbeddingFast()
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (inner_attn_ln): Identity()
          (inner_attn): FlashAttention()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): SwiGLU(
          (w1): Linear(in_features=1024, out_features=2730, bias=True)
          (w2): Linear(in_features=1024, out_features=2730, bias=True)
          (act): SiLU()
          (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
          (w3): Linear(in_features=2730, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1-23): 23 x Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (rope): VisionRotaryEmbeddingFast()
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (inner_attn_ln): Identity()
          (inner_attn): FlashAttention()
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): SwiGLU(
          (w1): Linear(in_features=1024, out_features=2730, bias=True)
          (w2): Linear(in_features=1024, out_features=2730, bias=True)
          (act): SiLU()
          (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
          (w3): Linear(in_features=2730, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (grid_mask): GridMask()
  (query_pos): Sequential(
    (0): Linear(in_features=396, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (time_embedding): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (ego_pose_pe): MLN(
    (reduce): Sequential(
      (0): Linear(in_features=156, out_features=256, bias=True)
      (1): ReLU()
    )
    (gamma): Linear(in_features=256, out_features=256, bias=True)
    (beta): Linear(in_features=256, out_features=256, bias=True)
    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
  )
  (map_head): OrionHeadM(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (cls_branches): ModuleList(
      (0-5): 6 x Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=6, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0-5): 6 x Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=33, bias=True)
      )
    )
    (input_projection): Linear(in_features=1024, out_features=256, bias=True)
    (output_projection): Linear(in_features=256, out_features=4096, bias=True)
    (reference_points_lane): Linear(in_features=256, out_features=3, bias=True)
    (points_embedding_lane): Embedding(11, 256)
    (instance_embedding_lane): Embedding(1800, 256)
    (query_embedding): Embedding(256, 256)
    (loss_dir): PtsDirCosLoss()
    (transformer): PETRTemporalTransformer(
      (query_decoder): PETRTransformerDecoder(
        (_layers): ModuleList(
          (0-5): 6 x PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
    )
    (query_pos): Sequential(
      (0): Linear(in_features=396, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (time_embedding): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (ego_pose_pe): MLN(
      (reduce): Sequential(
        (0): Linear(in_features=156, out_features=256, bias=True)
        (1): ReLU()
      )
      (gamma): Linear(in_features=256, out_features=256, bias=True)
      (beta): Linear(in_features=256, out_features=256, bias=True)
      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
    )
  )
  (position_encoder): Sequential(
    (0): Linear(in_features=192, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=256, bias=True)
  )
  (lm_head): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlavaLlamaForCausalLM(
        (model): LlavaLlamaModel(
          (embed_tokens): Embedding(32001, 4096)
          (layers): ModuleList(
            (0-31): 32 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=16, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=16, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=16, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=16, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=16, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=16, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=16, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=16, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
          )
          (norm): LlamaRMSNorm()
        )
        (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
      )
    )
  )
  (present_distribution): DistributionModule(
    (encoder): DistributionEncoder1DV2(
      (conv1): Conv1d(4096, 8192, kernel_size=(1,), stride=(1,))
      (conv2): Conv1d(8192, 8192, kernel_size=(1,), stride=(1,))
      (conv3): Conv1d(8192, 2048, kernel_size=(1,), stride=(1,))
      (relu): ReLU(inplace=True)
    )
    (last_conv): Sequential(
      (0): AdaptiveAvgPool1d(output_size=1)
      (1): Conv1d(2048, 64, kernel_size=(1,), stride=(1,))
    )
  )
  (future_distribution): DistributionModule(
    (encoder): DistributionEncoder1DV2(
      (conv1): Conv1d(4108, 8216, kernel_size=(1,), stride=(1,))
      (conv2): Conv1d(8216, 8216, kernel_size=(1,), stride=(1,))
      (conv3): Conv1d(8216, 2054, kernel_size=(1,), stride=(1,))
      (relu): ReLU(inplace=True)
    )
    (last_conv): Sequential(
      (0): AdaptiveAvgPool1d(output_size=1)
      (1): Conv1d(2054, 64, kernel_size=(1,), stride=(1,))
    )
  )
  (predict_model): PredictModel(
    (gru): GRU(32, 1024, num_layers=4)
    (linear1): Linear(in_features=1024, out_features=2048, bias=True)
    (linear2): Linear(in_features=2048, out_features=4096, bias=True)
    (linear3): Linear(in_features=4096, out_features=4096, bias=True)
    (relu): ReLU(inplace=True)
  )
  (ego_fut_decoder): Sequential(
    (0): Linear(in_features=8192, out_features=8192, bias=True)
    (1): ReLU()
    (2): Linear(in_features=8192, out_features=8192, bias=True)
    (3): ReLU()
    (4): Linear(in_features=8192, out_features=12, bias=True)
  )
  (loss_plan_reg): L1Loss()
  (loss_plan_bound): PlanMapBoundLoss()
  (loss_plan_col): PlanCollisionLoss()
  (loss_vae_gen): ProbabilisticLoss()
)
[rank0]: Traceback (most recent call last):
[rank0]:   File "./adzoo/orion/train.py", line 238, in <module>
[rank0]:     main()
[rank0]:   File "./adzoo/orion/train.py", line 226, in main
[rank0]:     custom_train_model(
[rank0]:   File "/mnt/sdb/swseo/Orion/adzoo/orion/apis/train.py", line 19, in custom_train_model
[rank0]:     custom_train_detector(
[rank0]:   File "/mnt/sdb/swseo/Orion/adzoo/orion/apis/mmdet_train.py", line 76, in custom_train_detector
[rank0]:     model.cuda(),
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 916, in cuda
[rank0]:     return self._apply(lambda t: t.cuda(device))
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 780, in _apply
[rank0]:     module._apply(fn)
[rank0]:   [Previous line repeated 6 more times]
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 805, in _apply
[rank0]:     param_applied = fn(param)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 916, in <lambda>
[rank0]:     return self._apply(lambda t: t.cuda(device))
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 187.88 MiB is free. Process 31791 has 23.44 GiB memory in use. Of the allocated memory 23.07 GiB is allocated by PyTorch, and 117.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
E0114 04:37:39.891989 139822850144064 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 15912) of binary: /mnt/sdb/swseo/orion_docker/envs/orion/bin/python
Traceback (most recent call last):
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launch.py", line 208, in <module>
    main()
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/typing_extensions.py", line 2853, in wrapper
    return arg(*args, **kwargs)
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launch.py", line 204, in main
    launch(args)
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in launch
    run(args)
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./adzoo/orion/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-14_04:37:39
  host      : LearningMachine3
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 15912)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
