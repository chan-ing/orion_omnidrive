nccl timeout value is set as 3600s!
2026-01-13 18:31:06,317 - mmdet - INFO - Environment info:
------------------------------------------------------------
MMCV: 0.0.1
------------------------------------------------------------

2026-01-13 18:31:08,816 - mmdet - INFO - Distributed training: True
2026-01-13 18:31:11,107 - mmdet - INFO - Config:
point_cloud_range = [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]
class_names = [
    'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
    'traffic_light', 'pedestrian', 'others'
]
dataset_type = 'B2DOrionDataset'
data_root = 'data/bench2drive'
input_modality = dict(
    use_lidar=False,
    use_camera=True,
    use_radar=False,
    use_map=False,
    use_external=True)
file_client_args = dict(backend='disk')
train_pipeline = [
    dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
    dict(type='PhotoMetricDistortionMultiViewImage'),
    dict(
        type='LoadAnnotations3D',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=True,
        with_light_state=True),
    dict(
        type='VADObjectRangeFilter',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='VADObjectNameFilter',
        classes=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ]),
    dict(
        type='LoadAnnoatationVQA',
        base_desc_path=None,
        tokenizer='ckpts/tiny_llama/',
        max_length=2048,
        use_gen_token=True,
        planning_qa_only=True,
        planning_qa_last=True),
    dict(
        type='ResizeCropFlipRotImage',
        data_aug_conf=dict(
            resize_lim=(0.37, 0.45),
            final_dim=(320, 640),
            bot_pct_lim=(0.0, 0.0),
            rot_lim=(0.0, 0.0),
            H=900,
            W=1600,
            rand_flip=False),
        training=True),
    dict(
        type='ResizeMultiview3D',
        img_scale=(640, 640),
        keep_ratio=False,
        multiscale_mode='value'),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='NormalizeMultiviewImage',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(
        type='PETRFormatBundle3D',
        class_names=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ],
        collect_keys=[
            'lidar2img', 'cam_intrinsic', 'timestamp', 'ego_pose',
            'ego_pose_inv', 'command'
        ]),
    dict(
        type='CustomCollect3D',
        keys=[
            'gt_bboxes_3d', 'gt_labels_3d', 'img', 'ego_his_trajs',
            'input_ids', 'gt_attr_labels', 'ego_fut_trajs', 'ego_fut_masks',
            'ego_fut_cmd', 'ego_lcf_feat', 'vlm_labels', 'can_bus',
            'traffic_state_mask', 'traffic_state', 'lidar2img',
            'cam_intrinsic', 'timestamp', 'ego_pose', 'ego_pose_inv', 'command'
        ])
]
test_pipeline = [
    dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
    dict(
        type='LoadAnnotations3D',
        with_bbox_3d=True,
        with_label_3d=True,
        with_attr_label=True),
    dict(
        type='VADObjectRangeFilter',
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
    dict(
        type='VADObjectNameFilter',
        classes=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ]),
    dict(
        type='ResizeCropFlipRotImage',
        data_aug_conf=dict(
            resize_lim=(0.37, 0.45),
            final_dim=(320, 640),
            bot_pct_lim=(0.0, 0.0),
            rot_lim=(0.0, 0.0),
            H=900,
            W=1600,
            rand_flip=False),
        training=False),
    dict(
        type='ResizeMultiview3D',
        img_scale=(640, 640),
        keep_ratio=False,
        multiscale_mode='value'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='LoadAnnoatationCriticalVQATest',
        load_type=['critical_qa'],
        tokenizer='ckpts/tiny_llama/',
        use_gen_token=True,
        max_length=2048),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1333, 800),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='PETRFormatBundle3D',
                collect_keys=[
                    'lidar2img', 'cam_intrinsic', 'timestamp', 'ego_pose',
                    'ego_pose_inv', 'command'
                ],
                class_names=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'img', 'ego_his_trajs',
                    'input_ids', 'gt_attr_labels', 'ego_fut_trajs',
                    'ego_fut_masks', 'ego_fut_cmd', 'ego_lcf_feat',
                    'vlm_labels', 'can_bus', 'fut_valid_flag', 'lidar2img',
                    'cam_intrinsic', 'timestamp', 'ego_pose', 'ego_pose_inv',
                    'command'
                ])
        ])
]
eval_pipeline = [
    dict(
        type='LoadPointsFromFile',
        coord_type='LIDAR',
        load_dim=5,
        use_dim=5,
        file_client_args=dict(backend='disk')),
    dict(
        type='LoadPointsFromMultiSweeps',
        sweeps_num=10,
        file_client_args=dict(backend='disk')),
    dict(
        type='DefaultFormatBundle3D',
        class_names=[
            'car', 'truck', 'trailer', 'bus', 'construction_vehicle',
            'bicycle', 'motorcycle', 'pedestrian', 'traffic_cone', 'barrier'
        ],
        with_label=False),
    dict(type='Collect3D', keys=['points'])
]
data = dict(
    samples_per_gpu=1,
    workers_per_gpu=4,
    train=dict(
        type='B2DOrionDataset',
        data_root='data/bench2drive',
        ann_file='data/infos/b2d_infos_train.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
            dict(type='PhotoMetricDistortionMultiViewImage'),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=True,
                with_light_state=True),
            dict(
                type='VADObjectRangeFilter',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='VADObjectNameFilter',
                classes=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ]),
            dict(
                type='LoadAnnoatationVQA',
                base_desc_path=None,
                tokenizer='ckpts/tiny_llama/',
                max_length=2048,
                use_gen_token=True,
                planning_qa_only=True,
                planning_qa_last=True),
            dict(
                type='ResizeCropFlipRotImage',
                data_aug_conf=dict(
                    resize_lim=(0.37, 0.45),
                    final_dim=(320, 640),
                    bot_pct_lim=(0.0, 0.0),
                    rot_lim=(0.0, 0.0),
                    H=900,
                    W=1600,
                    rand_flip=False),
                training=True),
            dict(
                type='ResizeMultiview3D',
                img_scale=(640, 640),
                keep_ratio=False,
                multiscale_mode='value'),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(
                type='PETRFormatBundle3D',
                class_names=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ],
                collect_keys=[
                    'lidar2img', 'cam_intrinsic', 'timestamp', 'ego_pose',
                    'ego_pose_inv', 'command'
                ]),
            dict(
                type='CustomCollect3D',
                keys=[
                    'gt_bboxes_3d', 'gt_labels_3d', 'img', 'ego_his_trajs',
                    'input_ids', 'gt_attr_labels', 'ego_fut_trajs',
                    'ego_fut_masks', 'ego_fut_cmd', 'ego_lcf_feat',
                    'vlm_labels', 'can_bus', 'traffic_state_mask',
                    'traffic_state', 'lidar2img', 'cam_intrinsic', 'timestamp',
                    'ego_pose', 'ego_pose_inv', 'command'
                ])
        ],
        classes=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=False,
        box_type_3d='LiDAR',
        seq_mode=True,
        seq_split_num=1,
        name_mapping=dict({
            'vehicle.bh.crossbike':
            'bicycle',
            'vehicle.diamondback.century':
            'bicycle',
            'vehicle.gazelle.omafiets':
            'bicycle',
            'vehicle.audi.etron':
            'car',
            'vehicle.chevrolet.impala':
            'car',
            'vehicle.dodge.charger_2020':
            'car',
            'vehicle.dodge.charger_police':
            'car',
            'vehicle.dodge.charger_police_2020':
            'car',
            'vehicle.lincoln.mkz_2017':
            'car',
            'vehicle.lincoln.mkz_2020':
            'car',
            'vehicle.mini.cooper_s_2021':
            'car',
            'vehicle.mercedes.coupe_2020':
            'car',
            'vehicle.ford.mustang':
            'car',
            'vehicle.nissan.patrol_2021':
            'car',
            'vehicle.audi.tt':
            'car',
            'vehicle.ford.crown':
            'car',
            'vehicle.tesla.model3':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/FordCrown/SM_FordCrown_parked.SM_FordCrown_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Charger/SM_ChargerParked.SM_ChargerParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Lincoln/SM_LincolnParked.SM_LincolnParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/MercedesCCC/SM_MercedesCCC_Parked.SM_MercedesCCC_Parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Mini2021/SM_Mini2021_parked.SM_Mini2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/NissanPatrol2021/SM_NissanPatrol2021_parked.SM_NissanPatrol2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/TeslaM3/SM_TeslaM3_parked.SM_TeslaM3_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/VolkswagenT2/SM_VolkswagenT2_2021_Parked.SM_VolkswagenT2_2021_Parked':
            'van',
            'vehicle.ford.ambulance':
            'van',
            'vehicle.carlamotors.firetruck':
            'truck',
            'traffic.speed_limit.30':
            'traffic_sign',
            'traffic.speed_limit.40':
            'traffic_sign',
            'traffic.speed_limit.50':
            'traffic_sign',
            'traffic.speed_limit.60':
            'traffic_sign',
            'traffic.speed_limit.90':
            'traffic_sign',
            'traffic.speed_limit.120':
            'traffic_sign',
            'traffic.stop':
            'traffic_sign',
            'traffic.yield':
            'traffic_sign',
            'traffic.traffic_light':
            'traffic_light',
            'static.prop.warningconstruction':
            'traffic_cone',
            'static.prop.warningaccident':
            'traffic_cone',
            'static.prop.trafficwarning':
            'traffic_cone',
            'static.prop.constructioncone':
            'traffic_cone',
            'walker.pedestrian.0001':
            'pedestrian',
            'walker.pedestrian.0003':
            'pedestrian',
            'walker.pedestrian.0004':
            'pedestrian',
            'walker.pedestrian.0005':
            'pedestrian',
            'walker.pedestrian.0007':
            'pedestrian',
            'walker.pedestrian.0010':
            'pedestrian',
            'walker.pedestrian.0013':
            'pedestrian',
            'walker.pedestrian.0014':
            'pedestrian',
            'walker.pedestrian.0015':
            'pedestrian',
            'walker.pedestrian.0016':
            'pedestrian',
            'walker.pedestrian.0017':
            'pedestrian',
            'walker.pedestrian.0018':
            'pedestrian',
            'walker.pedestrian.0019':
            'pedestrian',
            'walker.pedestrian.0020':
            'pedestrian',
            'walker.pedestrian.0021':
            'pedestrian',
            'walker.pedestrian.0022':
            'pedestrian',
            'walker.pedestrian.0025':
            'pedestrian',
            'walker.pedestrian.0027':
            'pedestrian',
            'walker.pedestrian.0030':
            'pedestrian',
            'walker.pedestrian.0031':
            'pedestrian',
            'walker.pedestrian.0032':
            'pedestrian',
            'walker.pedestrian.0034':
            'pedestrian',
            'walker.pedestrian.0035':
            'pedestrian',
            'walker.pedestrian.0041':
            'pedestrian',
            'walker.pedestrian.0042':
            'pedestrian',
            'walker.pedestrian.0046':
            'pedestrian',
            'walker.pedestrian.0047':
            'pedestrian',
            'static.prop.dirtdebris01':
            'others',
            'static.prop.dirtdebris02':
            'others'
        }),
        map_root='data/bench2drive/maps',
        map_file='data/infos/b2d_map_infos.pkl',
        queue_length=1,
        past_frames=2,
        future_frames=6,
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        polyline_points_num=11),
    val=dict(
        type='B2DOrionDataset',
        data_root='data/bench2drive',
        ann_file='data/infos/b2d_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=True),
            dict(
                type='VADObjectRangeFilter',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='VADObjectNameFilter',
                classes=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ]),
            dict(
                type='ResizeCropFlipRotImage',
                data_aug_conf=dict(
                    resize_lim=(0.37, 0.45),
                    final_dim=(320, 640),
                    bot_pct_lim=(0.0, 0.0),
                    rot_lim=(0.0, 0.0),
                    H=900,
                    W=1600,
                    rand_flip=False),
                training=False),
            dict(
                type='ResizeMultiview3D',
                img_scale=(640, 640),
                keep_ratio=False,
                multiscale_mode='value'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnoatationCriticalVQATest',
                load_type=['critical_qa'],
                tokenizer='ckpts/tiny_llama/',
                use_gen_token=True,
                max_length=2048),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='PETRFormatBundle3D',
                        collect_keys=[
                            'lidar2img', 'cam_intrinsic', 'timestamp',
                            'ego_pose', 'ego_pose_inv', 'command'
                        ],
                        class_names=[
                            'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                            'traffic_cone', 'traffic_light', 'pedestrian',
                            'others'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'gt_bboxes_3d', 'gt_labels_3d', 'img',
                            'ego_his_trajs', 'input_ids', 'gt_attr_labels',
                            'ego_fut_trajs', 'ego_fut_masks', 'ego_fut_cmd',
                            'ego_lcf_feat', 'vlm_labels', 'can_bus',
                            'fut_valid_flag', 'lidar2img', 'cam_intrinsic',
                            'timestamp', 'ego_pose', 'ego_pose_inv', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        name_mapping=dict({
            'vehicle.bh.crossbike':
            'bicycle',
            'vehicle.diamondback.century':
            'bicycle',
            'vehicle.gazelle.omafiets':
            'bicycle',
            'vehicle.audi.etron':
            'car',
            'vehicle.chevrolet.impala':
            'car',
            'vehicle.dodge.charger_2020':
            'car',
            'vehicle.dodge.charger_police':
            'car',
            'vehicle.dodge.charger_police_2020':
            'car',
            'vehicle.lincoln.mkz_2017':
            'car',
            'vehicle.lincoln.mkz_2020':
            'car',
            'vehicle.mini.cooper_s_2021':
            'car',
            'vehicle.mercedes.coupe_2020':
            'car',
            'vehicle.ford.mustang':
            'car',
            'vehicle.nissan.patrol_2021':
            'car',
            'vehicle.audi.tt':
            'car',
            'vehicle.ford.crown':
            'car',
            'vehicle.tesla.model3':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/FordCrown/SM_FordCrown_parked.SM_FordCrown_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Charger/SM_ChargerParked.SM_ChargerParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Lincoln/SM_LincolnParked.SM_LincolnParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/MercedesCCC/SM_MercedesCCC_Parked.SM_MercedesCCC_Parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Mini2021/SM_Mini2021_parked.SM_Mini2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/NissanPatrol2021/SM_NissanPatrol2021_parked.SM_NissanPatrol2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/TeslaM3/SM_TeslaM3_parked.SM_TeslaM3_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/VolkswagenT2/SM_VolkswagenT2_2021_Parked.SM_VolkswagenT2_2021_Parked':
            'van',
            'vehicle.ford.ambulance':
            'van',
            'vehicle.carlamotors.firetruck':
            'truck',
            'traffic.speed_limit.30':
            'traffic_sign',
            'traffic.speed_limit.40':
            'traffic_sign',
            'traffic.speed_limit.50':
            'traffic_sign',
            'traffic.speed_limit.60':
            'traffic_sign',
            'traffic.speed_limit.90':
            'traffic_sign',
            'traffic.speed_limit.120':
            'traffic_sign',
            'traffic.stop':
            'traffic_sign',
            'traffic.yield':
            'traffic_sign',
            'traffic.traffic_light':
            'traffic_light',
            'static.prop.warningconstruction':
            'traffic_cone',
            'static.prop.warningaccident':
            'traffic_cone',
            'static.prop.trafficwarning':
            'traffic_cone',
            'static.prop.constructioncone':
            'traffic_cone',
            'walker.pedestrian.0001':
            'pedestrian',
            'walker.pedestrian.0003':
            'pedestrian',
            'walker.pedestrian.0004':
            'pedestrian',
            'walker.pedestrian.0005':
            'pedestrian',
            'walker.pedestrian.0007':
            'pedestrian',
            'walker.pedestrian.0010':
            'pedestrian',
            'walker.pedestrian.0013':
            'pedestrian',
            'walker.pedestrian.0014':
            'pedestrian',
            'walker.pedestrian.0015':
            'pedestrian',
            'walker.pedestrian.0016':
            'pedestrian',
            'walker.pedestrian.0017':
            'pedestrian',
            'walker.pedestrian.0018':
            'pedestrian',
            'walker.pedestrian.0019':
            'pedestrian',
            'walker.pedestrian.0020':
            'pedestrian',
            'walker.pedestrian.0021':
            'pedestrian',
            'walker.pedestrian.0022':
            'pedestrian',
            'walker.pedestrian.0025':
            'pedestrian',
            'walker.pedestrian.0027':
            'pedestrian',
            'walker.pedestrian.0030':
            'pedestrian',
            'walker.pedestrian.0031':
            'pedestrian',
            'walker.pedestrian.0032':
            'pedestrian',
            'walker.pedestrian.0034':
            'pedestrian',
            'walker.pedestrian.0035':
            'pedestrian',
            'walker.pedestrian.0041':
            'pedestrian',
            'walker.pedestrian.0042':
            'pedestrian',
            'walker.pedestrian.0046':
            'pedestrian',
            'walker.pedestrian.0047':
            'pedestrian',
            'static.prop.dirtdebris01':
            'others',
            'static.prop.dirtdebris02':
            'others'
        }),
        map_root='data/bench2drive/maps',
        map_file='data/infos/b2d_map_infos.pkl',
        queue_length=1,
        past_frames=2,
        future_frames=6,
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        polyline_points_num=11,
        eval_cfg=dict(
            dist_ths=[0.5, 1.0, 2.0, 4.0],
            dist_th_tp=2.0,
            min_recall=0.1,
            min_precision=0.1,
            mean_ap_weight=5,
            class_names=[
                'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                'traffic_cone', 'traffic_light', 'pedestrian'
            ],
            tp_metrics=['trans_err', 'scale_err', 'orient_err', 'vel_err'],
            err_name_maping=dict(
                trans_err='mATE',
                scale_err='mASE',
                orient_err='mAOE',
                vel_err='mAVE',
                attr_err='mAAE'),
            class_range=dict(
                car=(50, 50),
                van=(50, 50),
                truck=(50, 50),
                bicycle=(40, 40),
                traffic_sign=(30, 30),
                traffic_cone=(30, 30),
                traffic_light=(30, 30),
                pedestrian=(40, 40)))),
    test=dict(
        type='B2DOrionDataset',
        data_root='data/bench2drive',
        ann_file='data/infos/b2d_infos_val.pkl',
        pipeline=[
            dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
            dict(
                type='LoadAnnotations3D',
                with_bbox_3d=True,
                with_label_3d=True,
                with_attr_label=True),
            dict(
                type='VADObjectRangeFilter',
                point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
            dict(
                type='VADObjectNameFilter',
                classes=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ]),
            dict(
                type='ResizeCropFlipRotImage',
                data_aug_conf=dict(
                    resize_lim=(0.37, 0.45),
                    final_dim=(320, 640),
                    bot_pct_lim=(0.0, 0.0),
                    rot_lim=(0.0, 0.0),
                    H=900,
                    W=1600,
                    rand_flip=False),
                training=False),
            dict(
                type='ResizeMultiview3D',
                img_scale=(640, 640),
                keep_ratio=False,
                multiscale_mode='value'),
            dict(
                type='NormalizeMultiviewImage',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='PadMultiViewImage', size_divisor=32),
            dict(
                type='LoadAnnoatationCriticalVQATest',
                load_type=['critical_qa'],
                tokenizer='ckpts/tiny_llama/',
                use_gen_token=True,
                max_length=2048),
            dict(
                type='MultiScaleFlipAug3D',
                img_scale=(1333, 800),
                pts_scale_ratio=1,
                flip=False,
                transforms=[
                    dict(
                        type='PETRFormatBundle3D',
                        collect_keys=[
                            'lidar2img', 'cam_intrinsic', 'timestamp',
                            'ego_pose', 'ego_pose_inv', 'command'
                        ],
                        class_names=[
                            'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                            'traffic_cone', 'traffic_light', 'pedestrian',
                            'others'
                        ],
                        with_label=False),
                    dict(
                        type='CustomCollect3D',
                        keys=[
                            'gt_bboxes_3d', 'gt_labels_3d', 'img',
                            'ego_his_trajs', 'input_ids', 'gt_attr_labels',
                            'ego_fut_trajs', 'ego_fut_masks', 'ego_fut_cmd',
                            'ego_lcf_feat', 'vlm_labels', 'can_bus',
                            'fut_valid_flag', 'lidar2img', 'cam_intrinsic',
                            'timestamp', 'ego_pose', 'ego_pose_inv', 'command'
                        ])
                ])
        ],
        classes=[
            'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
            'traffic_light', 'pedestrian', 'others'
        ],
        modality=dict(
            use_lidar=False,
            use_camera=True,
            use_radar=False,
            use_map=False,
            use_external=True),
        test_mode=True,
        box_type_3d='LiDAR',
        name_mapping=dict({
            'vehicle.bh.crossbike':
            'bicycle',
            'vehicle.diamondback.century':
            'bicycle',
            'vehicle.gazelle.omafiets':
            'bicycle',
            'vehicle.audi.etron':
            'car',
            'vehicle.chevrolet.impala':
            'car',
            'vehicle.dodge.charger_2020':
            'car',
            'vehicle.dodge.charger_police':
            'car',
            'vehicle.dodge.charger_police_2020':
            'car',
            'vehicle.lincoln.mkz_2017':
            'car',
            'vehicle.lincoln.mkz_2020':
            'car',
            'vehicle.mini.cooper_s_2021':
            'car',
            'vehicle.mercedes.coupe_2020':
            'car',
            'vehicle.ford.mustang':
            'car',
            'vehicle.nissan.patrol_2021':
            'car',
            'vehicle.audi.tt':
            'car',
            'vehicle.ford.crown':
            'car',
            'vehicle.tesla.model3':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/FordCrown/SM_FordCrown_parked.SM_FordCrown_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Charger/SM_ChargerParked.SM_ChargerParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Lincoln/SM_LincolnParked.SM_LincolnParked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/MercedesCCC/SM_MercedesCCC_Parked.SM_MercedesCCC_Parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Mini2021/SM_Mini2021_parked.SM_Mini2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/NissanPatrol2021/SM_NissanPatrol2021_parked.SM_NissanPatrol2021_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/TeslaM3/SM_TeslaM3_parked.SM_TeslaM3_parked':
            'car',
            '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/VolkswagenT2/SM_VolkswagenT2_2021_Parked.SM_VolkswagenT2_2021_Parked':
            'van',
            'vehicle.ford.ambulance':
            'van',
            'vehicle.carlamotors.firetruck':
            'truck',
            'traffic.speed_limit.30':
            'traffic_sign',
            'traffic.speed_limit.40':
            'traffic_sign',
            'traffic.speed_limit.50':
            'traffic_sign',
            'traffic.speed_limit.60':
            'traffic_sign',
            'traffic.speed_limit.90':
            'traffic_sign',
            'traffic.speed_limit.120':
            'traffic_sign',
            'traffic.stop':
            'traffic_sign',
            'traffic.yield':
            'traffic_sign',
            'traffic.traffic_light':
            'traffic_light',
            'static.prop.warningconstruction':
            'traffic_cone',
            'static.prop.warningaccident':
            'traffic_cone',
            'static.prop.trafficwarning':
            'traffic_cone',
            'static.prop.constructioncone':
            'traffic_cone',
            'walker.pedestrian.0001':
            'pedestrian',
            'walker.pedestrian.0003':
            'pedestrian',
            'walker.pedestrian.0004':
            'pedestrian',
            'walker.pedestrian.0005':
            'pedestrian',
            'walker.pedestrian.0007':
            'pedestrian',
            'walker.pedestrian.0010':
            'pedestrian',
            'walker.pedestrian.0013':
            'pedestrian',
            'walker.pedestrian.0014':
            'pedestrian',
            'walker.pedestrian.0015':
            'pedestrian',
            'walker.pedestrian.0016':
            'pedestrian',
            'walker.pedestrian.0017':
            'pedestrian',
            'walker.pedestrian.0018':
            'pedestrian',
            'walker.pedestrian.0019':
            'pedestrian',
            'walker.pedestrian.0020':
            'pedestrian',
            'walker.pedestrian.0021':
            'pedestrian',
            'walker.pedestrian.0022':
            'pedestrian',
            'walker.pedestrian.0025':
            'pedestrian',
            'walker.pedestrian.0027':
            'pedestrian',
            'walker.pedestrian.0030':
            'pedestrian',
            'walker.pedestrian.0031':
            'pedestrian',
            'walker.pedestrian.0032':
            'pedestrian',
            'walker.pedestrian.0034':
            'pedestrian',
            'walker.pedestrian.0035':
            'pedestrian',
            'walker.pedestrian.0041':
            'pedestrian',
            'walker.pedestrian.0042':
            'pedestrian',
            'walker.pedestrian.0046':
            'pedestrian',
            'walker.pedestrian.0047':
            'pedestrian',
            'static.prop.dirtdebris01':
            'others',
            'static.prop.dirtdebris02':
            'others'
        }),
        map_root='data/bench2drive/maps',
        map_file='data/infos/b2d_map_infos.pkl',
        queue_length=1,
        past_frames=2,
        future_frames=6,
        point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        polyline_points_num=11,
        eval_cfg=dict(
            dist_ths=[0.5, 1.0, 2.0, 4.0],
            dist_th_tp=2.0,
            min_recall=0.1,
            min_precision=0.1,
            mean_ap_weight=5,
            class_names=[
                'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                'traffic_cone', 'traffic_light', 'pedestrian'
            ],
            tp_metrics=['trans_err', 'scale_err', 'orient_err', 'vel_err'],
            err_name_maping=dict(
                trans_err='mATE',
                scale_err='mASE',
                orient_err='mAOE',
                vel_err='mAVE',
                attr_err='mAAE'),
            class_range=dict(
                car=(50, 50),
                van=(50, 50),
                truck=(50, 50),
                bicycle=(40, 40),
                traffic_sign=(30, 30),
                traffic_cone=(30, 30),
                traffic_light=(30, 30),
                pedestrian=(40, 40)))),
    shuffler_sampler=dict(
        type='InfiniteGroupEachSampleInBatchSampler',
        seq_split_num=10,
        warmup_split_num=80,
        num_iters_to_seq=7336),
    nonshuffler_sampler=dict(type='DistributedSampler'))
evaluation = dict(
    interval=51352,
    pipeline=[
        dict(type='LoadMultiViewImageFromFilesInCeph', to_float32=True),
        dict(
            type='LoadAnnotations3D',
            with_bbox_3d=True,
            with_label_3d=True,
            with_attr_label=True),
        dict(
            type='VADObjectRangeFilter',
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]),
        dict(
            type='VADObjectNameFilter',
            classes=[
                'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                'traffic_cone', 'traffic_light', 'pedestrian', 'others'
            ]),
        dict(
            type='ResizeCropFlipRotImage',
            data_aug_conf=dict(
                resize_lim=(0.37, 0.45),
                final_dim=(320, 640),
                bot_pct_lim=(0.0, 0.0),
                rot_lim=(0.0, 0.0),
                H=900,
                W=1600,
                rand_flip=False),
            training=False),
        dict(
            type='ResizeMultiview3D',
            img_scale=(640, 640),
            keep_ratio=False,
            multiscale_mode='value'),
        dict(
            type='NormalizeMultiviewImage',
            mean=[123.675, 116.28, 103.53],
            std=[58.395, 57.12, 57.375],
            to_rgb=True),
        dict(type='PadMultiViewImage', size_divisor=32),
        dict(
            type='LoadAnnoatationCriticalVQATest',
            load_type=['critical_qa'],
            tokenizer='ckpts/tiny_llama/',
            use_gen_token=True,
            max_length=2048),
        dict(
            type='MultiScaleFlipAug3D',
            img_scale=(1333, 800),
            pts_scale_ratio=1,
            flip=False,
            transforms=[
                dict(
                    type='PETRFormatBundle3D',
                    collect_keys=[
                        'lidar2img', 'cam_intrinsic', 'timestamp', 'ego_pose',
                        'ego_pose_inv', 'command'
                    ],
                    class_names=[
                        'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                        'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                    ],
                    with_label=False),
                dict(
                    type='CustomCollect3D',
                    keys=[
                        'gt_bboxes_3d', 'gt_labels_3d', 'img', 'ego_his_trajs',
                        'input_ids', 'gt_attr_labels', 'ego_fut_trajs',
                        'ego_fut_masks', 'ego_fut_cmd', 'ego_lcf_feat',
                        'vlm_labels', 'can_bus', 'fut_valid_flag', 'lidar2img',
                        'cam_intrinsic', 'timestamp', 'ego_pose',
                        'ego_pose_inv', 'command'
                    ])
            ])
    ])
checkpoint_config = dict(interval=7336, max_keep_ckpts=3)
log_config = dict(
    interval=10,
    hooks=[dict(type='TextLoggerHook'),
           dict(type='TensorboardLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
work_dir = 'adzoo/orion/work_dirs/orion_stage2_train/'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
backbone_norm_cfg = dict(type='LN', requires_grad=True)
voxel_size = [0.2, 0.2, 8]
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
map_classes = [
    'Broken', 'Solid', 'SolidSolid', 'Center', 'TrafficLight', 'StopSign'
]
queue_length = 1
map_fixed_ptsnum_per_gt_line = 11
map_eval_use_same_gt_sample_num_flag = True
map_num_classes = 6
past_frames = 2
future_frames = 6
_dim_ = 256
_pos_dim_ = 128
_ffn_dim_ = 512
ida_aug_conf = dict(
    resize_lim=(0.37, 0.45),
    final_dim=(320, 640),
    bot_pct_lim=(0.0, 0.0),
    rot_lim=(0.0, 0.0),
    H=900,
    W=1600,
    rand_flip=False)
occflow_grid_conf = dict(
    xbound=[-50.0, 50.0, 0.5],
    ybound=[-50.0, 50.0, 0.5],
    zbound=[-10.0, 10.0, 20.0])
NameMapping = dict({
    'vehicle.bh.crossbike':
    'bicycle',
    'vehicle.diamondback.century':
    'bicycle',
    'vehicle.gazelle.omafiets':
    'bicycle',
    'vehicle.audi.etron':
    'car',
    'vehicle.chevrolet.impala':
    'car',
    'vehicle.dodge.charger_2020':
    'car',
    'vehicle.dodge.charger_police':
    'car',
    'vehicle.dodge.charger_police_2020':
    'car',
    'vehicle.lincoln.mkz_2017':
    'car',
    'vehicle.lincoln.mkz_2020':
    'car',
    'vehicle.mini.cooper_s_2021':
    'car',
    'vehicle.mercedes.coupe_2020':
    'car',
    'vehicle.ford.mustang':
    'car',
    'vehicle.nissan.patrol_2021':
    'car',
    'vehicle.audi.tt':
    'car',
    'vehicle.ford.crown':
    'car',
    'vehicle.tesla.model3':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/FordCrown/SM_FordCrown_parked.SM_FordCrown_parked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Charger/SM_ChargerParked.SM_ChargerParked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Lincoln/SM_LincolnParked.SM_LincolnParked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/MercedesCCC/SM_MercedesCCC_Parked.SM_MercedesCCC_Parked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/Mini2021/SM_Mini2021_parked.SM_Mini2021_parked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/NissanPatrol2021/SM_NissanPatrol2021_parked.SM_NissanPatrol2021_parked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/TeslaM3/SM_TeslaM3_parked.SM_TeslaM3_parked':
    'car',
    '/Game/Carla/Static/Car/4Wheeled/ParkedVehicles/VolkswagenT2/SM_VolkswagenT2_2021_Parked.SM_VolkswagenT2_2021_Parked':
    'van',
    'vehicle.ford.ambulance':
    'van',
    'vehicle.carlamotors.firetruck':
    'truck',
    'traffic.speed_limit.30':
    'traffic_sign',
    'traffic.speed_limit.40':
    'traffic_sign',
    'traffic.speed_limit.50':
    'traffic_sign',
    'traffic.speed_limit.60':
    'traffic_sign',
    'traffic.speed_limit.90':
    'traffic_sign',
    'traffic.speed_limit.120':
    'traffic_sign',
    'traffic.stop':
    'traffic_sign',
    'traffic.yield':
    'traffic_sign',
    'traffic.traffic_light':
    'traffic_light',
    'static.prop.warningconstruction':
    'traffic_cone',
    'static.prop.warningaccident':
    'traffic_cone',
    'static.prop.trafficwarning':
    'traffic_cone',
    'static.prop.constructioncone':
    'traffic_cone',
    'walker.pedestrian.0001':
    'pedestrian',
    'walker.pedestrian.0003':
    'pedestrian',
    'walker.pedestrian.0004':
    'pedestrian',
    'walker.pedestrian.0005':
    'pedestrian',
    'walker.pedestrian.0007':
    'pedestrian',
    'walker.pedestrian.0010':
    'pedestrian',
    'walker.pedestrian.0013':
    'pedestrian',
    'walker.pedestrian.0014':
    'pedestrian',
    'walker.pedestrian.0015':
    'pedestrian',
    'walker.pedestrian.0016':
    'pedestrian',
    'walker.pedestrian.0017':
    'pedestrian',
    'walker.pedestrian.0018':
    'pedestrian',
    'walker.pedestrian.0019':
    'pedestrian',
    'walker.pedestrian.0020':
    'pedestrian',
    'walker.pedestrian.0021':
    'pedestrian',
    'walker.pedestrian.0022':
    'pedestrian',
    'walker.pedestrian.0025':
    'pedestrian',
    'walker.pedestrian.0027':
    'pedestrian',
    'walker.pedestrian.0030':
    'pedestrian',
    'walker.pedestrian.0031':
    'pedestrian',
    'walker.pedestrian.0032':
    'pedestrian',
    'walker.pedestrian.0034':
    'pedestrian',
    'walker.pedestrian.0035':
    'pedestrian',
    'walker.pedestrian.0041':
    'pedestrian',
    'walker.pedestrian.0042':
    'pedestrian',
    'walker.pedestrian.0046':
    'pedestrian',
    'walker.pedestrian.0047':
    'pedestrian',
    'static.prop.dirtdebris01':
    'others',
    'static.prop.dirtdebris02':
    'others'
})
eval_cfg = dict(
    dist_ths=[0.5, 1.0, 2.0, 4.0],
    dist_th_tp=2.0,
    min_recall=0.1,
    min_precision=0.1,
    mean_ap_weight=5,
    class_names=[
        'car', 'van', 'truck', 'bicycle', 'traffic_sign', 'traffic_cone',
        'traffic_light', 'pedestrian'
    ],
    tp_metrics=['trans_err', 'scale_err', 'orient_err', 'vel_err'],
    err_name_maping=dict(
        trans_err='mATE',
        scale_err='mASE',
        orient_err='mAOE',
        vel_err='mAVE',
        attr_err='mAAE'),
    class_range=dict(
        car=(50, 50),
        van=(50, 50),
        truck=(50, 50),
        bicycle=(40, 40),
        traffic_sign=(30, 30),
        traffic_cone=(30, 30),
        traffic_light=(30, 30),
        pedestrian=(40, 40)))
use_memory = True
num_gpus = 32
batch_size = 1
num_iters_per_epoch = 7336
num_epochs = 6
llm_path = 'ckpts/tiny_llama/'
use_gen_token = True
use_col_loss = True
collect_keys = [
    'lidar2img', 'cam_intrinsic', 'timestamp', 'ego_pose', 'ego_pose_inv',
    'command'
]
model = dict(
    type='Orion',
    save_path='./results_planning_only/',
    use_grid_mask=True,
    frozen=False,
    use_lora=True,
    tokenizer='ckpts/tiny_llama/',
    lm_head='ckpts/tiny_llama/',
    use_gen_token=True,
    use_diff_decoder=False,
    use_col_loss=True,
    loss_plan_reg=dict(type='L1Loss', loss_weight=3.0),
    loss_plan_bound=dict(
        type='PlanMapBoundLoss', loss_weight=3.0, dis_thresh=1.0),
    loss_plan_col=dict(type='PlanCollisionLoss', loss_weight=1.0),
    loss_vae_gen=dict(type='ProbabilisticLoss', loss_weight=3.0),
    img_backbone=dict(
        type='EVAViT',
        img_size=640,
        patch_size=16,
        window_size=16,
        in_chans=3,
        embed_dim=1024,
        depth=24,
        num_heads=16,
        mlp_ratio=2.6666666666666665,
        window_block_indexes=[
            0, 1, 3, 4, 6, 7, 9, 10, 12, 13, 15, 16, 18, 19, 21, 22
        ],
        qkv_bias=True,
        drop_path_rate=0.3,
        flash_attn=True,
        with_cp=True,
        frozen=False),
    map_head=dict(
        type='OrionHeadM',
        num_classes=6,
        in_channels=1024,
        out_dims=4096,
        memory_len=600,
        with_mask=True,
        topk_proposals=300,
        num_lane=1800,
        num_lanes_one2one=300,
        k_one2many=5,
        lambda_one2many=1.0,
        num_extra=256,
        n_control=11,
        pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
        code_weights=[1.0, 1.0],
        score_threshold=0.2,
        transformer=dict(
            type='PETRTemporalTransformer',
            input_dimension=256,
            output_dimension=256,
            num_layers=6,
            embed_dims=256,
            num_heads=8,
            feedforward_dims=2048,
            dropout=0.1,
            with_cp=True,
            flash_attn=True),
        train_cfg=dict(
            assigner=dict(
                type='LaneHungarianAssigner',
                cls_cost=dict(type='FocalLossCost', weight=1.5),
                reg_cost=dict(type='LaneL1Cost', weight=0.02),
                iou_cost=dict(type='IoUCost', weight=0.0))),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=1.5),
        loss_bbox=dict(type='L1Loss', loss_weight=0.02),
        loss_dir=dict(type='PtsDirCosLoss', loss_weight=0.0)),
    pts_bbox_head=dict(
        type='OrionHead',
        num_classes=9,
        in_channels=1024,
        out_dims=4096,
        num_query=600,
        with_mask=True,
        memory_len=600,
        topk_proposals=300,
        num_propagated=300,
        num_extra=256,
        n_control=11,
        match_with_velo=False,
        pred_traffic_light_state=True,
        use_col_loss=True,
        use_memory=True,
        scalar=10,
        noise_scale=1.0,
        dn_weight=1.0,
        split=0.75,
        use_pe=False,
        motion_transformer_decoder=dict(
            type='OrionTransformerDecoder',
            num_layers=1,
            embed_dims=256,
            num_heads=8,
            dropout=0.0,
            feedforward_dims=512,
            with_cp=True,
            flash_attn=True,
            return_intermediate=False),
        code_weights=[2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
        score_threshold=0.2,
        class_agnostic_nms=dict(
            classes=[0, 1, 2, 3, 4, 5, 6, 7, 8],
            compensate=[0, 0, 0.3, 0, 0, 0, 0, 0.3, 0],
            pre_max_size=1000,
            post_max_size=300,
            nms_thr=0.1),
        memory_decoder_transformer=dict(
            type='OrionTransformerDecoder',
            num_layers=1,
            embed_dims=256,
            num_heads=8,
            dropout=0.0,
            feedforward_dims=512,
            with_cp=True,
            flash_attn=True,
            return_intermediate=False),
        transformer=dict(
            type='PETRTemporalTransformer',
            input_dimension=256,
            output_dimension=256,
            num_layers=6,
            embed_dims=256,
            num_heads=8,
            feedforward_dims=2048,
            dropout=0.1,
            with_cp=True,
            flash_attn=True),
        bbox_coder=dict(
            type='CustomNMSFreeCoder',
            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            max_num=300,
            voxel_size=[0.2, 0.2, 8],
            num_classes=9),
        loss_cls=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_traffic=dict(
            type='FocalLoss',
            use_sigmoid=True,
            gamma=2.0,
            alpha=0.25,
            loss_weight=2.0),
        loss_bbox=dict(type='L1Loss', loss_weight=0.25),
        loss_iou=dict(type='GIoULoss', loss_weight=0.0)),
    train_cfg=dict(
        pts=dict(
            grid_size=[512, 512, 1],
            voxel_size=[0.2, 0.2, 8],
            point_cloud_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
            out_size_factor=4,
            assigner=dict(
                type='HungarianAssigner3D',
                cls_cost=dict(type='FocalLossCost', weight=2.0),
                reg_cost=dict(type='BBox3DL1Cost', weight=0.25),
                iou_cost=dict(type='IoUCost', weight=0.0),
                pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0]))))
info_root = 'data/infos'
map_root = 'data/bench2drive/maps'
map_file = 'data/infos/b2d_map_infos.pkl'
ann_file_train = 'data/infos/b2d_infos_train.pkl'
ann_file_val = 'data/infos/b2d_infos_val.pkl'
ann_file_test = 'data/infos/b2d_infos_val.pkl'
inference_only_pipeline = [
    dict(
        type='LoadMultiViewImageFromFilesInCeph',
        to_float32=True,
        file_client_args=dict(backend='disk'),
        img_root='data/bench2drive'),
    dict(
        type='NormalizeMultiviewImage',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='PadMultiViewImage', size_divisor=32),
    dict(
        type='MultiScaleFlipAug3D',
        img_scale=(1600, 900),
        pts_scale_ratio=1,
        flip=False,
        transforms=[
            dict(
                type='DefaultFormatBundle3D',
                class_names=[
                    'car', 'van', 'truck', 'bicycle', 'traffic_sign',
                    'traffic_cone', 'traffic_light', 'pedestrian', 'others'
                ],
                with_label=False),
            dict(
                type='CustomCollect3D',
                keys=['img', 'timestamp', 'l2g_r_mat', 'l2g_t', 'command'])
        ])
]
optimizer = dict(
    constructor='LearningRateDecayOptimizerConstructor',
    type='AdamW',
    lr=8e-05,
    betas=(0.9, 0.999),
    weight_decay=1e-05,
    paramwise_cfg=dict(
        decay_rate=0.9,
        head_decay_rate=4.0,
        lm_head_decay_rate=0.1,
        decay_type='vit_wise',
        num_layers=24))
optimizer_config = dict(
    type='Fp16OptimizerHook',
    loss_scale='dynamic',
    grad_clip=dict(max_norm=35, norm_type=2))
lr_config = dict(
    policy='CosineAnnealing',
    warmup='linear',
    warmup_iters=500,
    warmup_ratio=0.3333333333333333,
    min_lr_ratio=0.001)
find_unused_parameters = False
runner = dict(type='IterBasedRunner', max_iters=44016)
gpu_ids = range(0, 1)

2026-01-13 18:31:11,107 - mmdet - INFO - Set random seed to 0, deterministic: True
======== shape of rope freq torch.Size([256, 64]) ========
======== shape of rope freq torch.Size([1600, 64]) ========
Some weights of the model checkpoint at ckpts/tiny_llama/ were not used when initializing LlavaLlamaForCausalLM: ['model.mm_projector.query_decoder._layers.5.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.mlp.w2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.8.attn.proj.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.4._layers.0.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.4._layers.3.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.5.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.6.attn.proj.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.attn.v_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.10.norm2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.1.norm1.bias', 'model.mm_projector.query_decoder._layers.0.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.mlp.w3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.5.attn.v_proj.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.0.attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.3.mlp.w3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.norm1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.mlp.ffn_ln.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.2.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.6.attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.attn.q_bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.4._layers.3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.attn.k_proj.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.4._layers.3.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.5.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.2.norm1.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.6.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.9.attn.k_proj.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.0.attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.6.mlp.w2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.vision_tower.blocks.10.mlp.w3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.7.mlp.w3.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.7.attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.3.attn.proj.weight', 'model.mm_projector.output_projection.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.7.norm2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.6.attn.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.mlp.w3.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.4._layers.0.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.8.attn.proj.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.2.attn.in_proj_weight', 'model.vision_tower.vision_tower.vision_tower.blocks.3.mlp.ffn_ln.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.7.mlp.w3.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.2.attn.in_proj_weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.attn.v_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.5.attn.v_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.attn.proj.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.2.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.norm2.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.0.attn.in_proj_weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.mlp.w2.bias', 'model.mm_projector.query_decoder._layers.0.transformer_layers.0.attn.out_proj.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.1.attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.6.norm1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.2.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.vision_tower.blocks.8.attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.3.attn.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.3.attn.v_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.5.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.vision_tower.blocks.7.attn.proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.vision_tower.blocks.6.attn.v_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.10.attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.7.attn.rope.freqs_sin', 'model.mm_projector.query_decoder._layers.3.transformer_layers.2.attn.in_proj_weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.3.attn.q_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.0.mlp.w3.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.mlp.w3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.3.norm1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.3.mlp.w1.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.2.attn.out_proj.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.0.attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.1.attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.9.mlp.w2.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.mlp.w2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.attn.proj.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.2.mlp.w1.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.0.attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.2.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.vision_tower.blocks.5.norm2.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.1.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.5.mlp.w3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.7.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.vision_tower.blocks.3.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.vision_tower.blocks.10.attn.q_bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.2.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.0.norm2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.6.mlp.w2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.norm1.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.5.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.norm2.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.4._layers.0.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.0.attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.attn.q_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.6.attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.11.attn.q_proj.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.3.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.1.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.3.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.4._layers.3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.1.attn.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.attn.q_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.mlp.w2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.attn.v_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.2.mlp.w3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.attn.v_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.8.attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.6.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.2.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.5.attn.proj.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.1.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.mlp.w3.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.2.attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.mlp.w2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.mlp.w3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.6.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.vision_tower.blocks.1.attn.rope.freqs_sin', 'model.mm_projector.query_decoder._layers.1.transformer_layers.0.attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.8.attn.q_bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.0.attn.in_proj_weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.0.attn.in_proj_weight', 'model.vision_tower.vision_tower.vision_tower.blocks.7.attn.q_bias', 'model.vision_tower.vision_tower.vision_tower.rope_glb.freqs_cos', 'model.mm_projector.query_decoder._layers.3.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.7.mlp.w2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.9.norm2.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.2.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.8.mlp.w1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.4.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.vision_tower.blocks.8.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.mlp.w3.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.0.attn.out_proj.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.4.norm1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.7.mlp.w2.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.0.attn.in_proj_weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.3.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.4._layers.3.weight', 'model.mm_projector.output_projection.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.4._layers.0.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.attn.q_bias', 'model.vision_tower.vision_tower.vision_tower.pos_embed', 'model.vision_tower.vision_tower.vision_tower.patch_embed.proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.8.mlp.w3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.5.attn.q_proj.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.4._layers.0.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.8.attn.rope.freqs_cos', 'model.mm_projector.query_decoder._layers.2.transformer_layers.2.attn.in_proj_weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.1.weight', 'model.mm_projector.query_decoder._layers.5.transformer_layers.2.attn.in_proj_bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.5.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.vision_tower.blocks.6.attn.v_proj.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.rope_glb.freqs_sin', 'model.mm_projector.query_decoder._layers.0.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.8.attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.8.norm1.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.4._layers.0.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.9.attn.v_proj.weight', 'model.mm_projector.input_projection.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.3.mlp.w2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_tower.rope_win.freqs_cos', 'model.mm_projector.query_decoder._layers.1.transformer_layers.1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.6.norm2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.attn.v_proj.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.norm1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.4.mlp.w2.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.2.attn.in_proj_weight', 'model.vision_tower.vision_tower.vision_tower.blocks.7.mlp.w1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.4.mlp.w1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.attn.v_bias', 'model.mm_projector.query_decoder._layers.0.transformer_layers.4._layers.0.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.0.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.mlp.w3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.10.mlp.w3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.5.mlp.w3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.10.attn.k_proj.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.4._layers.0.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.2.attn.in_proj_weight', 'model.vision_tower.vision_tower.vision_tower.blocks.1.attn.rope.freqs_cos', 'model.mm_projector.query_decoder._layers.3.transformer_layers.3.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.5.bias', 'model.mm_projector.query_embedding.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.5.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.vision_tower.blocks.7.attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.5.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.0.mlp.w1.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.2.attn.out_proj.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.0.attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.mlp.w2.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.5.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.norm2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.5.norm1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.mlp.w2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.10.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.vision_tower.blocks.8.mlp.w2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.2.attn.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.3.norm2.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.5.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.11.attn.q_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.7.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.attn.v_bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.attn.proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.7.attn.q_proj.weight', 'model.mm_projector.query_decoder._layers.1.transformer_layers.2.attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.attn.proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.mlp.w3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.7.attn.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.attn.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.0.norm2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.norm1.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.8.norm2.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.5.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.attn.v_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.norm2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.10.attn.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.0.norm1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.attn.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.7.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.0.attn.in_proj_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.0.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.7.norm1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.8.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.8.mlp.w3.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.8.norm1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.norm2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.mlp.w2.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.5.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.5.attn.q_bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.4._layers.0.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.3.attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.11.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.vision_tower.blocks.2.attn.q_proj.weight', 'model.mm_projector.input_projection.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.5.mlp.w2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.9.mlp.w3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.mlp.w2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.11.norm1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.6.mlp.w3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.0.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.0.transformer_layers.3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.8.mlp.w2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.9.mlp.w1.bias', 'model.mm_projector.query_decoder._layers.4.transformer_layers.3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.1.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.7.attn.v_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.mlp.w2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.5.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.attn.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.6.mlp.w3.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.norm1.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.7.mlp.ffn_ln.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.4._layers.3.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.0.attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.mlp.w1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.3.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.6.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.4._layers.0.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.4._layers.0.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.5.mlp.w1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.4.norm1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.9.attn.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.3.attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.mlp.w2.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.6.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.vision_tower.blocks.3.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.vision_tower.blocks.5.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.9.norm1.bias', 'model.vision_tower.vision_tower.vision_tower.patch_embed.proj.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.2.attn.q_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.norm2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.3.norm1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.8.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.vision_tower.rope_win.freqs_sin', 'model.vision_tower.vision_tower.vision_tower.blocks.3.mlp.w3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.11.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.7.norm1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.attn.proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.vision_tower.blocks.3.attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.11.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.vision_tower.blocks.9.mlp.ffn_ln.bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.2.attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.5.attn.proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.8.attn.rope.freqs_sin', 'model.vision_tower.vision_tower.vision_tower.blocks.7.norm2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.6.mlp.w1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.mlp.w1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.8.norm2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.3.norm2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.attn.rope.freqs_cos', 'model.vision_tower.vision_tower.vision_tower.blocks.4.norm2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.norm1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.0.attn.proj.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.3.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.4._layers.3.bias', 'model.mm_projector.query_decoder._layers.0.transformer_layers.0.attn.out_proj.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.5.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.1.mlp.w3.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.1.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.0.norm1.bias', 'model.mm_projector.query_decoder._layers.2.transformer_layers.4._layers.3.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.5.norm1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.norm2.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.2.attn.out_proj.weight', 'model.mm_projector.query_decoder._layers.0.transformer_layers.4._layers.0.weight', 'model.mm_projector.query_decoder._layers.3.transformer_layers.2.attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.norm2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.2.attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.4.mlp.w2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.mlp.ffn_ln.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.2.norm2.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.5.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.1.mlp.w1.bias', 'model.mm_projector.query_decoder._layers.1.transformer_layers.1.bias', 'model.mm_projector.query_decoder._layers.3.transformer_layers.0.attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.6.norm2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.4.attn.proj.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.2.attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.mlp.w1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.5.mlp.w2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.attn.q_proj.weight', 'model.mm_projector.query_decoder._layers.4.transformer_layers.1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.8.attn.v_bias', 'model.vision_tower.vision_tower.vision_tower.blocks.3.mlp.w1.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.9.attn.rope.freqs_sin', 'model.mm_projector.query_decoder._layers.0.transformer_layers.0.attn.in_proj_weight', 'model.vision_tower.vision_tower.vision_tower.blocks.5.attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.10.mlp.ffn_ln.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.6.attn.q_bias', 'model.mm_projector.query_decoder._layers.5.transformer_layers.5.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.6.norm1.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.3.mlp.w2.bias', 'model.vision_tower.vision_tower.vision_tower.blocks.11.attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_tower.blocks.5.norm2.weight', 'model.mm_projector.query_decoder._layers.2.transformer_layers.0.attn.in_proj_weight']
- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at ckpts/tiny_llama/ and are newly initialized: ['model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'weighted_mask', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2026-01-13 18:31:46,093 - mmdet - INFO - Model:
Orion(
  (pts_bbox_head): OrionHead(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (traj_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=12, bias=True)
      )
    )
    (traj_cls_branches): ModuleList(
      (0): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=512, out_features=1, bias=True)
      )
    )
    (cls_branches): ModuleList(
      (0-5): 6 x Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=9, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0-5): 6 x Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (input_projection): Linear(in_features=1024, out_features=256, bias=True)
    (output_projection): Linear(in_features=256, out_features=4096, bias=True)
    (reference_points): Embedding(600, 3)
    (pseudo_reference_points): Embedding(300, 3)
    (query_embedding): Embedding(256, 256)
    (can_bus_embed): Sequential(
      (0): Linear(in_features=89, out_features=1024, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1024, out_features=4096, bias=True)
    )
    (loss_traj): L1Loss()
    (loss_traj_cls): FocalLoss()
    (loss_iou): GIoULoss()
    (transformer): PETRTemporalTransformer(
      (query_decoder): PETRTransformerDecoder(
        (_layers): ModuleList(
          (0-5): 6 x PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
    )
    (motion_decoder): OrionTransformerDecoder(
      (_layers): ModuleList(
        (0): OrionTransformerDecoderLayer(
          (transformer_layers): ModuleList(
            (0): MultiHeadAttentionwDropout(
              (attn): FlashMHA(
                (inner_attn): FlashAttention()
                (out_proj): Linear(in_features=256, out_features=256, bias=False)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): FFN(
              (_layers): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=512, out_features=256, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (motion_mode_query): Embedding(6, 256)
    (memory_query): Embedding(16, 256)
    (scene_time_embedding): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (memory_decoder_cq): OrionTransformerDecoder(
      (_layers): ModuleList(
        (0): OrionTransformerDecoderLayer(
          (transformer_layers): ModuleList(
            (0): MultiHeadAttentionwDropout(
              (attn): FlashMHA(
                (inner_attn): FlashAttention()
                (out_proj): Linear(in_features=256, out_features=256, bias=False)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): FFN(
              (_layers): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=512, out_features=256, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (memory_decoder_mq): OrionTransformerDecoder(
      (_layers): ModuleList(
        (0): OrionTransformerDecoderLayer(
          (transformer_layers): ModuleList(
            (0): MultiHeadAttentionwDropout(
              (attn): FlashMHA(
                (inner_attn): FlashAttention()
                (out_proj): Linear(in_features=256, out_features=256, bias=False)
              )
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (2): FFN(
              (_layers): Sequential(
                (0): Linear(in_features=256, out_features=512, bias=True)
                (1): ReLU(inplace=True)
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=512, out_features=256, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
            (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (loss_traffic): FocalLoss()
    (tl_branches): ModuleList(
      (0-5): 6 x Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (query_pos): Sequential(
      (0): Linear(in_features=396, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (time_embedding): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (ego_pose_pe): MLN(
      (reduce): Sequential(
        (0): Linear(in_features=156, out_features=256, bias=True)
        (1): ReLU()
      )
      (gamma): Linear(in_features=256, out_features=256, bias=True)
      (beta): Linear(in_features=256, out_features=256, bias=True)
      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
    )
  )
  (img_backbone): EVAViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
    )
    (rope_win): VisionRotaryEmbeddingFast()
    (rope_glb): VisionRotaryEmbeddingFast()
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (rope): VisionRotaryEmbeddingFast()
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (inner_attn_ln): Identity()
          (inner_attn): FlashAttention()
        )
        (drop_path): Identity()
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): SwiGLU(
          (w1): Linear(in_features=1024, out_features=2730, bias=True)
          (w2): Linear(in_features=1024, out_features=2730, bias=True)
          (act): SiLU()
          (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
          (w3): Linear(in_features=2730, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1-23): 23 x Block(
        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (rope): VisionRotaryEmbeddingFast()
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (inner_attn_ln): Identity()
          (inner_attn): FlashAttention()
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (mlp): SwiGLU(
          (w1): Linear(in_features=1024, out_features=2730, bias=True)
          (w2): Linear(in_features=1024, out_features=2730, bias=True)
          (act): SiLU()
          (ffn_ln): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)
          (w3): Linear(in_features=2730, out_features=1024, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (grid_mask): GridMask()
  (query_pos): Sequential(
    (0): Linear(in_features=396, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
  )
  (time_embedding): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (ego_pose_pe): MLN(
    (reduce): Sequential(
      (0): Linear(in_features=156, out_features=256, bias=True)
      (1): ReLU()
    )
    (gamma): Linear(in_features=256, out_features=256, bias=True)
    (beta): Linear(in_features=256, out_features=256, bias=True)
    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
  )
  (map_head): OrionHeadM(
    (loss_cls): FocalLoss()
    (loss_bbox): L1Loss()
    (cls_branches): ModuleList(
      (0-5): 6 x Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=256, out_features=256, bias=True)
        (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=256, out_features=6, bias=True)
      )
    )
    (reg_branches): ModuleList(
      (0-5): 6 x Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=256, bias=True)
        (3): ReLU()
        (4): Linear(in_features=256, out_features=33, bias=True)
      )
    )
    (input_projection): Linear(in_features=1024, out_features=256, bias=True)
    (output_projection): Linear(in_features=256, out_features=4096, bias=True)
    (reference_points_lane): Linear(in_features=256, out_features=3, bias=True)
    (points_embedding_lane): Embedding(11, 256)
    (instance_embedding_lane): Embedding(1800, 256)
    (query_embedding): Embedding(256, 256)
    (loss_dir): PtsDirCosLoss()
    (transformer): PETRTemporalTransformer(
      (query_decoder): PETRTransformerDecoder(
        (_layers): ModuleList(
          (0-5): 6 x PETRTransformerDecoderLayer(
            (transformer_layers): ModuleList(
              (0): MultiHeadAttentionwDropout(
                (attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (2): MultiHeadAttentionwDropout(
                (attn): FlashMHA(
                  (inner_attn): FlashAttention()
                  (out_proj): Linear(in_features=256, out_features=256, bias=True)
                )
                (proj_drop): Dropout(p=0.1, inplace=False)
              )
              (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (4): FFN(
                (_layers): Sequential(
                  (0): Linear(in_features=256, out_features=2048, bias=True)
                  (1): ReLU(inplace=True)
                  (2): Dropout(p=0.1, inplace=False)
                  (3): Linear(in_features=2048, out_features=256, bias=True)
                  (4): Dropout(p=0.1, inplace=False)
                )
              )
              (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
    )
    (query_pos): Sequential(
      (0): Linear(in_features=396, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (time_embedding): Sequential(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (ego_pose_pe): MLN(
      (reduce): Sequential(
        (0): Linear(in_features=156, out_features=256, bias=True)
        (1): ReLU()
      )
      (gamma): Linear(in_features=256, out_features=256, bias=True)
      (beta): Linear(in_features=256, out_features=256, bias=True)
      (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
    )
  )
  (position_encoder): Sequential(
    (0): Linear(in_features=192, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=256, bias=True)
  )
  (lm_head): PeftModelForCausalLM(
    (base_model): LoraModel(
      (model): LlavaLlamaForCausalLM(
        (model): LlavaLlamaModel(
          (embed_tokens): Embedding(32001, 2048)
          (layers): ModuleList(
            (0-21): 22 x LlamaDecoderLayer(
              (self_attn): LlamaAttention(
                (q_proj): lora.Linear(
                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=2048, out_features=16, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=16, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (k_proj): lora.Linear(
                  (base_layer): Linear(in_features=2048, out_features=256, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=2048, out_features=16, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=16, out_features=256, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (v_proj): lora.Linear(
                  (base_layer): Linear(in_features=2048, out_features=256, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=2048, out_features=16, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=16, out_features=256, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (o_proj): lora.Linear(
                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=2048, out_features=16, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=16, out_features=2048, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                  (lora_magnitude_vector): ModuleDict()
                )
                (rotary_emb): LlamaRotaryEmbedding()
              )
              (mlp): LlamaMLP(
                (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
                (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
                (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
                (act_fn): SiLUActivation()
              )
              (input_layernorm): LlamaRMSNorm()
              (post_attention_layernorm): LlamaRMSNorm()
            )
          )
          (norm): LlamaRMSNorm()
        )
        (lm_head): Linear(in_features=2048, out_features=32001, bias=False)
      )
    )
  )
  (present_distribution): DistributionModule(
    (encoder): DistributionEncoder1DV2(
      (conv1): Conv1d(4096, 8192, kernel_size=(1,), stride=(1,))
      (conv2): Conv1d(8192, 8192, kernel_size=(1,), stride=(1,))
      (conv3): Conv1d(8192, 2048, kernel_size=(1,), stride=(1,))
      (relu): ReLU(inplace=True)
    )
    (last_conv): Sequential(
      (0): AdaptiveAvgPool1d(output_size=1)
      (1): Conv1d(2048, 64, kernel_size=(1,), stride=(1,))
    )
  )
  (future_distribution): DistributionModule(
    (encoder): DistributionEncoder1DV2(
      (conv1): Conv1d(4108, 8216, kernel_size=(1,), stride=(1,))
      (conv2): Conv1d(8216, 8216, kernel_size=(1,), stride=(1,))
      (conv3): Conv1d(8216, 2054, kernel_size=(1,), stride=(1,))
      (relu): ReLU(inplace=True)
    )
    (last_conv): Sequential(
      (0): AdaptiveAvgPool1d(output_size=1)
      (1): Conv1d(2054, 64, kernel_size=(1,), stride=(1,))
    )
  )
  (predict_model): PredictModel(
    (gru): GRU(32, 1024, num_layers=4)
    (linear1): Linear(in_features=1024, out_features=2048, bias=True)
    (linear2): Linear(in_features=2048, out_features=4096, bias=True)
    (linear3): Linear(in_features=4096, out_features=4096, bias=True)
    (relu): ReLU(inplace=True)
  )
  (ego_fut_decoder): Sequential(
    (0): Linear(in_features=8192, out_features=8192, bias=True)
    (1): ReLU()
    (2): Linear(in_features=8192, out_features=8192, bias=True)
    (3): ReLU()
    (4): Linear(in_features=8192, out_features=12, bias=True)
  )
  (loss_plan_reg): L1Loss()
  (loss_plan_bound): PlanMapBoundLoss()
  (loss_plan_col): PlanCollisionLoss()
  (loss_vae_gen): ProbabilisticLoss()
)
{'decay_rate': 0.9, 'head_decay_rate': 4.0, 'lm_head_decay_rate': 0.1, 'decay_type': 'vit_wise', 'num_layers': 24}
Build LearningRateDecayOptimizerConstructor vit_wise 0.900000 - 26
Param groups = {
  "layer_25_decay": {
    "param_names": [
      "pts_bbox_head.traj_branches.0.0.weight",
      "pts_bbox_head.traj_branches.0.2.weight",
      "pts_bbox_head.traj_branches.0.4.weight",
      "pts_bbox_head.traj_cls_branches.0.0.weight",
      "pts_bbox_head.traj_cls_branches.0.3.weight",
      "pts_bbox_head.traj_cls_branches.0.6.weight",
      "pts_bbox_head.cls_branches.0.0.weight",
      "pts_bbox_head.cls_branches.0.3.weight",
      "pts_bbox_head.cls_branches.0.6.weight",
      "pts_bbox_head.reg_branches.0.0.weight",
      "pts_bbox_head.reg_branches.0.2.weight",
      "pts_bbox_head.reg_branches.0.4.weight",
      "pts_bbox_head.input_projection.weight",
      "pts_bbox_head.output_projection.weight",
      "pts_bbox_head.reference_points.weight",
      "pts_bbox_head.query_embedding.weight",
      "pts_bbox_head.can_bus_embed.0.weight",
      "pts_bbox_head.can_bus_embed.2.weight",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.weight",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.weight",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.weight",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.weight",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.weight",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_weight",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.weight",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.weight",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.weight",
      "pts_bbox_head.motion_decoder._layers.0.transformer_layers.0.attn.in_proj_weight",
      "pts_bbox_head.motion_decoder._layers.0.transformer_layers.0.attn.out_proj.weight",
      "pts_bbox_head.motion_decoder._layers.0.transformer_layers.2._layers.0.weight",
      "pts_bbox_head.motion_decoder._layers.0.transformer_layers.2._layers.3.weight",
      "pts_bbox_head.motion_mode_query.weight",
      "pts_bbox_head.memory_query.weight",
      "pts_bbox_head.scene_time_embedding.0.weight",
      "pts_bbox_head.memory_decoder_cq._layers.0.transformer_layers.0.attn.in_proj_weight",
      "pts_bbox_head.memory_decoder_cq._layers.0.transformer_layers.0.attn.out_proj.weight",
      "pts_bbox_head.memory_decoder_cq._layers.0.transformer_layers.2._layers.0.weight",
      "pts_bbox_head.memory_decoder_cq._layers.0.transformer_layers.2._layers.3.weight",
      "pts_bbox_head.memory_decoder_mq._layers.0.transformer_layers.0.attn.in_proj_weight",
      "pts_bbox_head.memory_decoder_mq._layers.0.transformer_layers.0.attn.out_proj.weight",
      "pts_bbox_head.memory_decoder_mq._layers.0.transformer_layers.2._layers.0.weight",
      "pts_bbox_head.memory_decoder_mq._layers.0.transformer_layers.2._layers.3.weight",
      "pts_bbox_head.tl_branches.0.0.weight",
      "pts_bbox_head.tl_branches.0.3.weight",
      "pts_bbox_head.tl_branches.0.6.weight",
      "pts_bbox_head.query_pos.0.weight",
      "pts_bbox_head.query_pos.2.weight",
      "pts_bbox_head.time_embedding.0.weight",
      "pts_bbox_head.ego_pose_pe.reduce.0.weight",
      "pts_bbox_head.ego_pose_pe.gamma.weight",
      "pts_bbox_head.ego_pose_pe.beta.weight",
      "map_head.cls_branches.0.0.weight",
      "map_head.cls_branches.0.3.weight",
      "map_head.cls_branches.0.6.weight",
      "map_head.reg_branches.0.0.weight",
      "map_head.reg_branches.0.2.weight",
      "map_head.reg_branches.0.4.weight",
      "map_head.input_projection.weight",
      "map_head.output_projection.weight",
      "map_head.reference_points_lane.weight",
      "map_head.points_embedding_lane.weight",
      "map_head.instance_embedding_lane.weight",
      "map_head.query_embedding.weight",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.weight",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.weight",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.weight",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.weight",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.weight",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.weight",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.weight",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.weight",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.weight",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.weight",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_weight",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.weight",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.weight",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.weight",
      "position_encoder.0.weight",
      "position_encoder.2.weight",
      "lm_head.base_model.model.model.embed_tokens.weight",
      "lm_head.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight",
      "lm_head.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight",
      "lm_head.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight",
      "lm_head.base_model.model.lm_head.weight",
      "present_distribution.encoder.conv1.weight",
      "present_distribution.encoder.conv2.weight",
      "present_distribution.encoder.conv3.weight",
      "present_distribution.last_conv.1.weight",
      "future_distribution.encoder.conv1.weight",
      "future_distribution.encoder.conv2.weight",
      "future_distribution.encoder.conv3.weight",
      "future_distribution.last_conv.1.weight",
      "predict_model.gru.weight_ih_l0",
      "predict_model.gru.weight_hh_l0",
      "predict_model.gru.weight_ih_l1",
      "predict_model.gru.weight_hh_l1",
      "predict_model.gru.weight_ih_l2",
      "predict_model.gru.weight_hh_l2",
      "predict_model.gru.weight_ih_l3",
      "predict_model.gru.weight_hh_l3",
      "predict_model.linear1.weight",
      "predict_model.linear2.weight",
      "predict_model.linear3.weight",
      "ego_fut_decoder.0.weight",
      "ego_fut_decoder.2.weight",
      "ego_fut_decoder.4.weight"
    ],
    "lr_scale": 4.0,
    "lr": 0.00032,
    "weight_decay": 1e-05
  },
  "layer_25_no_decay": {
    "param_names": [
      "pts_bbox_head.traj_branches.0.0.bias",
      "pts_bbox_head.traj_branches.0.2.bias",
      "pts_bbox_head.traj_branches.0.4.bias",
      "pts_bbox_head.traj_cls_branches.0.0.bias",
      "pts_bbox_head.traj_cls_branches.0.1.weight",
      "pts_bbox_head.traj_cls_branches.0.1.bias",
      "pts_bbox_head.traj_cls_branches.0.3.bias",
      "pts_bbox_head.traj_cls_branches.0.4.weight",
      "pts_bbox_head.traj_cls_branches.0.4.bias",
      "pts_bbox_head.traj_cls_branches.0.6.bias",
      "pts_bbox_head.cls_branches.0.0.bias",
      "pts_bbox_head.cls_branches.0.1.weight",
      "pts_bbox_head.cls_branches.0.1.bias",
      "pts_bbox_head.cls_branches.0.3.bias",
      "pts_bbox_head.cls_branches.0.4.weight",
      "pts_bbox_head.cls_branches.0.4.bias",
      "pts_bbox_head.cls_branches.0.6.bias",
      "pts_bbox_head.reg_branches.0.0.bias",
      "pts_bbox_head.reg_branches.0.2.bias",
      "pts_bbox_head.reg_branches.0.4.bias",
      "pts_bbox_head.input_projection.bias",
      "pts_bbox_head.output_projection.bias",
      "pts_bbox_head.can_bus_embed.0.bias",
      "pts_bbox_head.can_bus_embed.2.bias",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.1.weight",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.1.bias",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.bias",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.5.weight",
      "pts_bbox_head.transformer.query_decoder._layers.0.transformer_layers.5.bias",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.1.weight",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.1.bias",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.bias",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.5.weight",
      "pts_bbox_head.transformer.query_decoder._layers.1.transformer_layers.5.bias",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.1.weight",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.1.bias",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.bias",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.5.weight",
      "pts_bbox_head.transformer.query_decoder._layers.2.transformer_layers.5.bias",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.1.weight",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.1.bias",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.bias",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.5.weight",
      "pts_bbox_head.transformer.query_decoder._layers.3.transformer_layers.5.bias",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.1.weight",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.1.bias",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.bias",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.5.weight",
      "pts_bbox_head.transformer.query_decoder._layers.4.transformer_layers.5.bias",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.1.weight",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.1.bias",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_bias",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.bias",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.3.weight",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.bias",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.bias",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.5.weight",
      "pts_bbox_head.transformer.query_decoder._layers.5.transformer_layers.5.bias",
      "pts_bbox_head.motion_decoder._layers.0.transformer_layers.1.weight",
      "pts_bbox_head.motion_decoder._layers.0.transformer_layers.1.bias",
      "pts_bbox_head.motion_decoder._layers.0.transformer_layers.2._layers.0.bias",
      "pts_bbox_head.motion_decoder._layers.0.transformer_layers.2._layers.3.bias",
      "pts_bbox_head.motion_decoder._layers.0.transformer_layers.3.weight",
      "pts_bbox_head.motion_decoder._layers.0.transformer_layers.3.bias",
      "pts_bbox_head.scene_time_embedding.0.bias",
      "pts_bbox_head.scene_time_embedding.1.weight",
      "pts_bbox_head.scene_time_embedding.1.bias",
      "pts_bbox_head.memory_decoder_cq._layers.0.transformer_layers.1.weight",
      "pts_bbox_head.memory_decoder_cq._layers.0.transformer_layers.1.bias",
      "pts_bbox_head.memory_decoder_cq._layers.0.transformer_layers.2._layers.0.bias",
      "pts_bbox_head.memory_decoder_cq._layers.0.transformer_layers.2._layers.3.bias",
      "pts_bbox_head.memory_decoder_cq._layers.0.transformer_layers.3.weight",
      "pts_bbox_head.memory_decoder_cq._layers.0.transformer_layers.3.bias",
      "pts_bbox_head.memory_decoder_mq._layers.0.transformer_layers.1.weight",
      "pts_bbox_head.memory_decoder_mq._layers.0.transformer_layers.1.bias",
      "pts_bbox_head.memory_decoder_mq._layers.0.transformer_layers.2._layers.0.bias",
      "pts_bbox_head.memory_decoder_mq._layers.0.transformer_layers.2._layers.3.bias",
      "pts_bbox_head.memory_decoder_mq._layers.0.transformer_layers.3.weight",
      "pts_bbox_head.memory_decoder_mq._layers.0.transformer_layers.3.bias",
      "pts_bbox_head.tl_branches.0.0.bias",
      "pts_bbox_head.tl_branches.0.1.weight",
      "pts_bbox_head.tl_branches.0.1.bias",
      "pts_bbox_head.tl_branches.0.3.bias",
      "pts_bbox_head.tl_branches.0.4.weight",
      "pts_bbox_head.tl_branches.0.4.bias",
      "pts_bbox_head.tl_branches.0.6.bias",
      "pts_bbox_head.query_pos.0.bias",
      "pts_bbox_head.query_pos.2.bias",
      "pts_bbox_head.time_embedding.0.bias",
      "pts_bbox_head.time_embedding.1.weight",
      "pts_bbox_head.time_embedding.1.bias",
      "pts_bbox_head.ego_pose_pe.reduce.0.bias",
      "pts_bbox_head.ego_pose_pe.gamma.bias",
      "pts_bbox_head.ego_pose_pe.beta.bias",
      "map_head.cls_branches.0.0.bias",
      "map_head.cls_branches.0.1.weight",
      "map_head.cls_branches.0.1.bias",
      "map_head.cls_branches.0.3.bias",
      "map_head.cls_branches.0.4.weight",
      "map_head.cls_branches.0.4.bias",
      "map_head.cls_branches.0.6.bias",
      "map_head.reg_branches.0.0.bias",
      "map_head.reg_branches.0.2.bias",
      "map_head.reg_branches.0.4.bias",
      "map_head.input_projection.bias",
      "map_head.output_projection.bias",
      "map_head.reference_points_lane.bias",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.0.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.1.weight",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.1.bias",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.2.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.3.weight",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.3.bias",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.0.bias",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.4._layers.3.bias",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.5.weight",
      "map_head.transformer.query_decoder._layers.0.transformer_layers.5.bias",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.0.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.1.weight",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.1.bias",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.2.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.3.weight",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.3.bias",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.0.bias",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.4._layers.3.bias",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.5.weight",
      "map_head.transformer.query_decoder._layers.1.transformer_layers.5.bias",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.0.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.1.weight",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.1.bias",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.2.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.3.weight",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.3.bias",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.0.bias",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.4._layers.3.bias",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.5.weight",
      "map_head.transformer.query_decoder._layers.2.transformer_layers.5.bias",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.0.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.1.weight",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.1.bias",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.2.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.3.weight",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.3.bias",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.0.bias",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.4._layers.3.bias",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.5.weight",
      "map_head.transformer.query_decoder._layers.3.transformer_layers.5.bias",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.0.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.1.weight",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.1.bias",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.2.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.3.weight",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.3.bias",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.0.bias",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.4._layers.3.bias",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.5.weight",
      "map_head.transformer.query_decoder._layers.4.transformer_layers.5.bias",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.0.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.1.weight",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.1.bias",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.in_proj_bias",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.2.attn.out_proj.bias",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.3.weight",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.3.bias",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.0.bias",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.4._layers.3.bias",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.5.weight",
      "map_head.transformer.query_decoder._layers.5.transformer_layers.5.bias",
      "position_encoder.0.bias",
      "position_encoder.2.bias",
      "present_distribution.encoder.conv1.bias",
      "present_distribution.encoder.conv2.bias",
      "present_distribution.encoder.conv3.bias",
      "present_distribution.last_conv.1.bias",
      "future_distribution.encoder.conv1.bias",
      "future_distribution.encoder.conv2.bias",
      "future_distribution.encoder.conv3.bias",
      "future_distribution.last_conv.1.bias",
      "predict_model.gru.bias_ih_l0",
      "predict_model.gru.bias_hh_l0",
      "predict_model.gru.bias_ih_l1",
      "predict_model.gru.bias_hh_l1",
      "predict_model.gru.bias_ih_l2",
      "predict_model.gru.bias_hh_l2",
      "predict_model.gru.bias_ih_l3",
      "predict_model.gru.bias_hh_l3",
      "predict_model.linear1.bias",
      "predict_model.linear2.bias",
      "predict_model.linear3.bias",
      "ego_fut_decoder.0.bias",
      "ego_fut_decoder.2.bias",
      "ego_fut_decoder.4.bias"
    ],
    "lr_scale": 4.0,
    "lr": 0.00032,
    "weight_decay": 0.0
  },
  "layer_0_decay": {
    "param_names": [
      "img_backbone.pos_embed",
      "img_backbone.patch_embed.proj.weight"
    ],
    "lr_scale": 0.0717897987691853,
    "lr": 5.7431839015348245e-06,
    "weight_decay": 1e-05
  },
  "layer_0_no_decay": {
    "param_names": [
      "img_backbone.patch_embed.proj.bias"
    ],
    "lr_scale": 0.0717897987691853,
    "lr": 5.7431839015348245e-06,
    "weight_decay": 0.0
  },
  "layer_1_no_decay": {
    "param_names": [
      "img_backbone.blocks.0.norm1.weight",
      "img_backbone.blocks.0.norm1.bias",
      "img_backbone.blocks.0.attn.q_bias",
      "img_backbone.blocks.0.attn.v_bias",
      "img_backbone.blocks.0.attn.proj.bias",
      "img_backbone.blocks.0.norm2.weight",
      "img_backbone.blocks.0.norm2.bias",
      "img_backbone.blocks.0.mlp.w1.bias",
      "img_backbone.blocks.0.mlp.w2.bias",
      "img_backbone.blocks.0.mlp.ffn_ln.weight",
      "img_backbone.blocks.0.mlp.ffn_ln.bias",
      "img_backbone.blocks.0.mlp.w3.bias"
    ],
    "lr_scale": 0.07976644307687256,
    "lr": 6.381315446149805e-06,
    "weight_decay": 0.0
  },
  "layer_1_decay": {
    "param_names": [
      "img_backbone.blocks.0.attn.q_proj.weight",
      "img_backbone.blocks.0.attn.k_proj.weight",
      "img_backbone.blocks.0.attn.v_proj.weight",
      "img_backbone.blocks.0.attn.proj.weight",
      "img_backbone.blocks.0.mlp.w1.weight",
      "img_backbone.blocks.0.mlp.w2.weight",
      "img_backbone.blocks.0.mlp.w3.weight"
    ],
    "lr_scale": 0.07976644307687256,
    "lr": 6.381315446149805e-06,
    "weight_decay": 1e-05
  },
  "layer_2_no_decay": {
    "param_names": [
      "img_backbone.blocks.1.norm1.weight",
      "img_backbone.blocks.1.norm1.bias",
      "img_backbone.blocks.1.attn.q_bias",
      "img_backbone.blocks.1.attn.v_bias",
      "img_backbone.blocks.1.attn.proj.bias",
      "img_backbone.blocks.1.norm2.weight",
      "img_backbone.blocks.1.norm2.bias",
      "img_backbone.blocks.1.mlp.w1.bias",
      "img_backbone.blocks.1.mlp.w2.bias",
      "img_backbone.blocks.1.mlp.ffn_ln.weight",
      "img_backbone.blocks.1.mlp.ffn_ln.bias",
      "img_backbone.blocks.1.mlp.w3.bias"
    ],
    "lr_scale": 0.08862938119652507,
    "lr": 7.090350495722006e-06,
    "weight_decay": 0.0
  },
  "layer_2_decay": {
    "param_names": [
      "img_backbone.blocks.1.attn.q_proj.weight",
      "img_backbone.blocks.1.attn.k_proj.weight",
      "img_backbone.blocks.1.attn.v_proj.weight",
      "img_backbone.blocks.1.attn.proj.weight",
      "img_backbone.blocks.1.mlp.w1.weight",
      "img_backbone.blocks.1.mlp.w2.weight",
      "img_backbone.blocks.1.mlp.w3.weight"
    ],
    "lr_scale": 0.08862938119652507,
    "lr": 7.090350495722006e-06,
    "weight_decay": 1e-05
  },
  "layer_3_no_decay": {
    "param_names": [
      "img_backbone.blocks.2.norm1.weight",
      "img_backbone.blocks.2.norm1.bias",
      "img_backbone.blocks.2.attn.q_bias",
      "img_backbone.blocks.2.attn.v_bias",
      "img_backbone.blocks.2.attn.proj.bias",
      "img_backbone.blocks.2.norm2.weight",
      "img_backbone.blocks.2.norm2.bias",
      "img_backbone.blocks.2.mlp.w1.bias",
      "img_backbone.blocks.2.mlp.w2.bias",
      "img_backbone.blocks.2.mlp.ffn_ln.weight",
      "img_backbone.blocks.2.mlp.ffn_ln.bias",
      "img_backbone.blocks.2.mlp.w3.bias"
    ],
    "lr_scale": 0.09847709021836118,
    "lr": 7.878167217468896e-06,
    "weight_decay": 0.0
  },
  "layer_3_decay": {
    "param_names": [
      "img_backbone.blocks.2.attn.q_proj.weight",
      "img_backbone.blocks.2.attn.k_proj.weight",
      "img_backbone.blocks.2.attn.v_proj.weight",
      "img_backbone.blocks.2.attn.proj.weight",
      "img_backbone.blocks.2.mlp.w1.weight",
      "img_backbone.blocks.2.mlp.w2.weight",
      "img_backbone.blocks.2.mlp.w3.weight"
    ],
    "lr_scale": 0.09847709021836118,
    "lr": 7.878167217468896e-06,
    "weight_decay": 1e-05
  },
  "layer_4_no_decay": {
    "param_names": [
      "img_backbone.blocks.3.norm1.weight",
      "img_backbone.blocks.3.norm1.bias",
      "img_backbone.blocks.3.attn.q_bias",
      "img_backbone.blocks.3.attn.v_bias",
      "img_backbone.blocks.3.attn.proj.bias",
      "img_backbone.blocks.3.norm2.weight",
      "img_backbone.blocks.3.norm2.bias",
      "img_backbone.blocks.3.mlp.w1.bias",
      "img_backbone.blocks.3.mlp.w2.bias",
      "img_backbone.blocks.3.mlp.ffn_ln.weight",
      "img_backbone.blocks.3.mlp.ffn_ln.bias",
      "img_backbone.blocks.3.mlp.w3.bias"
    ],
    "lr_scale": 0.10941898913151242,
    "lr": 8.753519130520995e-06,
    "weight_decay": 0.0
  },
  "layer_4_decay": {
    "param_names": [
      "img_backbone.blocks.3.attn.q_proj.weight",
      "img_backbone.blocks.3.attn.k_proj.weight",
      "img_backbone.blocks.3.attn.v_proj.weight",
      "img_backbone.blocks.3.attn.proj.weight",
      "img_backbone.blocks.3.mlp.w1.weight",
      "img_backbone.blocks.3.mlp.w2.weight",
      "img_backbone.blocks.3.mlp.w3.weight"
    ],
    "lr_scale": 0.10941898913151242,
    "lr": 8.753519130520995e-06,
    "weight_decay": 1e-05
  },
  "layer_5_no_decay": {
    "param_names": [
      "img_backbone.blocks.4.norm1.weight",
      "img_backbone.blocks.4.norm1.bias",
      "img_backbone.blocks.4.attn.q_bias",
      "img_backbone.blocks.4.attn.v_bias",
      "img_backbone.blocks.4.attn.proj.bias",
      "img_backbone.blocks.4.norm2.weight",
      "img_backbone.blocks.4.norm2.bias",
      "img_backbone.blocks.4.mlp.w1.bias",
      "img_backbone.blocks.4.mlp.w2.bias",
      "img_backbone.blocks.4.mlp.ffn_ln.weight",
      "img_backbone.blocks.4.mlp.ffn_ln.bias",
      "img_backbone.blocks.4.mlp.w3.bias"
    ],
    "lr_scale": 0.12157665459056935,
    "lr": 9.726132367245548e-06,
    "weight_decay": 0.0
  },
  "layer_5_decay": {
    "param_names": [
      "img_backbone.blocks.4.attn.q_proj.weight",
      "img_backbone.blocks.4.attn.k_proj.weight",
      "img_backbone.blocks.4.attn.v_proj.weight",
      "img_backbone.blocks.4.attn.proj.weight",
      "img_backbone.blocks.4.mlp.w1.weight",
      "img_backbone.blocks.4.mlp.w2.weight",
      "img_backbone.blocks.4.mlp.w3.weight"
    ],
    "lr_scale": 0.12157665459056935,
    "lr": 9.726132367245548e-06,
    "weight_decay": 1e-05
  },
  "layer_6_no_decay": {
    "param_names": [
      "img_backbone.blocks.5.norm1.weight",
      "img_backbone.blocks.5.norm1.bias",
      "img_backbone.blocks.5.attn.q_bias",
      "img_backbone.blocks.5.attn.v_bias",
      "img_backbone.blocks.5.attn.proj.bias",
      "img_backbone.blocks.5.norm2.weight",
      "img_backbone.blocks.5.norm2.bias",
      "img_backbone.blocks.5.mlp.w1.bias",
      "img_backbone.blocks.5.mlp.w2.bias",
      "img_backbone.blocks.5.mlp.ffn_ln.weight",
      "img_backbone.blocks.5.mlp.ffn_ln.bias",
      "img_backbone.blocks.5.mlp.w3.bias"
    ],
    "lr_scale": 0.13508517176729928,
    "lr": 1.0806813741383944e-05,
    "weight_decay": 0.0
  },
  "layer_6_decay": {
    "param_names": [
      "img_backbone.blocks.5.attn.q_proj.weight",
      "img_backbone.blocks.5.attn.k_proj.weight",
      "img_backbone.blocks.5.attn.v_proj.weight",
      "img_backbone.blocks.5.attn.proj.weight",
      "img_backbone.blocks.5.mlp.w1.weight",
      "img_backbone.blocks.5.mlp.w2.weight",
      "img_backbone.blocks.5.mlp.w3.weight"
    ],
    "lr_scale": 0.13508517176729928,
    "lr": 1.0806813741383944e-05,
    "weight_decay": 1e-05
  },
  "layer_7_no_decay": {
    "param_names": [
      "img_backbone.blocks.6.norm1.weight",
      "img_backbone.blocks.6.norm1.bias",
      "img_backbone.blocks.6.attn.q_bias",
      "img_backbone.blocks.6.attn.v_bias",
      "img_backbone.blocks.6.attn.proj.bias",
      "img_backbone.blocks.6.norm2.weight",
      "img_backbone.blocks.6.norm2.bias",
      "img_backbone.blocks.6.mlp.w1.bias",
      "img_backbone.blocks.6.mlp.w2.bias",
      "img_backbone.blocks.6.mlp.ffn_ln.weight",
      "img_backbone.blocks.6.mlp.ffn_ln.bias",
      "img_backbone.blocks.6.mlp.w3.bias"
    ],
    "lr_scale": 0.15009463529699918,
    "lr": 1.2007570823759936e-05,
    "weight_decay": 0.0
  },
  "layer_7_decay": {
    "param_names": [
      "img_backbone.blocks.6.attn.q_proj.weight",
      "img_backbone.blocks.6.attn.k_proj.weight",
      "img_backbone.blocks.6.attn.v_proj.weight",
      "img_backbone.blocks.6.attn.proj.weight",
      "img_backbone.blocks.6.mlp.w1.weight",
      "img_backbone.blocks.6.mlp.w2.weight",
      "img_backbone.blocks.6.mlp.w3.weight"
    ],
    "lr_scale": 0.15009463529699918,
    "lr": 1.2007570823759936e-05,
    "weight_decay": 1e-05
  },
  "layer_8_no_decay": {
    "param_names": [
      "img_backbone.blocks.7.norm1.weight",
      "img_backbone.blocks.7.norm1.bias",
      "img_backbone.blocks.7.attn.q_bias",
      "img_backbone.blocks.7.attn.v_bias",
      "img_backbone.blocks.7.attn.proj.bias",
      "img_backbone.blocks.7.norm2.weight",
      "img_backbone.blocks.7.norm2.bias",
      "img_backbone.blocks.7.mlp.w1.bias",
      "img_backbone.blocks.7.mlp.w2.bias",
      "img_backbone.blocks.7.mlp.ffn_ln.weight",
      "img_backbone.blocks.7.mlp.ffn_ln.bias",
      "img_backbone.blocks.7.mlp.w3.bias"
    ],
    "lr_scale": 0.16677181699666577,
    "lr": 1.3341745359733262e-05,
    "weight_decay": 0.0
  },
  "layer_8_decay": {
    "param_names": [
      "img_backbone.blocks.7.attn.q_proj.weight",
      "img_backbone.blocks.7.attn.k_proj.weight",
      "img_backbone.blocks.7.attn.v_proj.weight",
      "img_backbone.blocks.7.attn.proj.weight",
      "img_backbone.blocks.7.mlp.w1.weight",
      "img_backbone.blocks.7.mlp.w2.weight",
      "img_backbone.blocks.7.mlp.w3.weight"
    ],
    "lr_scale": 0.16677181699666577,
    "lr": 1.3341745359733262e-05,
    "weight_decay": 1e-05
  },
  "layer_9_no_decay": {
    "param_names": [
      "img_backbone.blocks.8.norm1.weight",
      "img_backbone.blocks.8.norm1.bias",
      "img_backbone.blocks.8.attn.q_bias",
      "img_backbone.blocks.8.attn.v_bias",
      "img_backbone.blocks.8.attn.proj.bias",
      "img_backbone.blocks.8.norm2.weight",
      "img_backbone.blocks.8.norm2.bias",
      "img_backbone.blocks.8.mlp.w1.bias",
      "img_backbone.blocks.8.mlp.w2.bias",
      "img_backbone.blocks.8.mlp.ffn_ln.weight",
      "img_backbone.blocks.8.mlp.ffn_ln.bias",
      "img_backbone.blocks.8.mlp.w3.bias"
    ],
    "lr_scale": 0.18530201888518416,
    "lr": 1.4824161510814735e-05,
    "weight_decay": 0.0
  },
  "layer_9_decay": {
    "param_names": [
      "img_backbone.blocks.8.attn.q_proj.weight",
      "img_backbone.blocks.8.attn.k_proj.weight",
      "img_backbone.blocks.8.attn.v_proj.weight",
      "img_backbone.blocks.8.attn.proj.weight",
      "img_backbone.blocks.8.mlp.w1.weight",
      "img_backbone.blocks.8.mlp.w2.weight",
      "img_backbone.blocks.8.mlp.w3.weight"
    ],
    "lr_scale": 0.18530201888518416,
    "lr": 1.4824161510814735e-05,
    "weight_decay": 1e-05
  },
  "layer_10_no_decay": {
    "param_names": [
      "img_backbone.blocks.9.norm1.weight",
      "img_backbone.blocks.9.norm1.bias",
      "img_backbone.blocks.9.attn.q_bias",
      "img_backbone.blocks.9.attn.v_bias",
      "img_backbone.blocks.9.attn.proj.bias",
      "img_backbone.blocks.9.norm2.weight",
      "img_backbone.blocks.9.norm2.bias",
      "img_backbone.blocks.9.mlp.w1.bias",
      "img_backbone.blocks.9.mlp.w2.bias",
      "img_backbone.blocks.9.mlp.ffn_ln.weight",
      "img_backbone.blocks.9.mlp.ffn_ln.bias",
      "img_backbone.blocks.9.mlp.w3.bias"
    ],
    "lr_scale": 0.20589113209464907,
    "lr": 1.6471290567571928e-05,
    "weight_decay": 0.0
  },
  "layer_10_decay": {
    "param_names": [
      "img_backbone.blocks.9.attn.q_proj.weight",
      "img_backbone.blocks.9.attn.k_proj.weight",
      "img_backbone.blocks.9.attn.v_proj.weight",
      "img_backbone.blocks.9.attn.proj.weight",
      "img_backbone.blocks.9.mlp.w1.weight",
      "img_backbone.blocks.9.mlp.w2.weight",
      "img_backbone.blocks.9.mlp.w3.weight"
    ],
    "lr_scale": 0.20589113209464907,
    "lr": 1.6471290567571928e-05,
    "weight_decay": 1e-05
  },
  "layer_11_no_decay": {
    "param_names": [
      "img_backbone.blocks.10.norm1.weight",
      "img_backbone.blocks.10.norm1.bias",
      "img_backbone.blocks.10.attn.q_bias",
      "img_backbone.blocks.10.attn.v_bias",
      "img_backbone.blocks.10.attn.proj.bias",
      "img_backbone.blocks.10.norm2.weight",
      "img_backbone.blocks.10.norm2.bias",
      "img_backbone.blocks.10.mlp.w1.bias",
      "img_backbone.blocks.10.mlp.w2.bias",
      "img_backbone.blocks.10.mlp.ffn_ln.weight",
      "img_backbone.blocks.10.mlp.ffn_ln.bias",
      "img_backbone.blocks.10.mlp.w3.bias"
    ],
    "lr_scale": 0.2287679245496101,
    "lr": 1.8301433963968808e-05,
    "weight_decay": 0.0
  },
  "layer_11_decay": {
    "param_names": [
      "img_backbone.blocks.10.attn.q_proj.weight",
      "img_backbone.blocks.10.attn.k_proj.weight",
      "img_backbone.blocks.10.attn.v_proj.weight",
      "img_backbone.blocks.10.attn.proj.weight",
      "img_backbone.blocks.10.mlp.w1.weight",
      "img_backbone.blocks.10.mlp.w2.weight",
      "img_backbone.blocks.10.mlp.w3.weight"
    ],
    "lr_scale": 0.2287679245496101,
    "lr": 1.8301433963968808e-05,
    "weight_decay": 1e-05
  },
  "layer_12_no_decay": {
    "param_names": [
      "img_backbone.blocks.11.norm1.weight",
      "img_backbone.blocks.11.norm1.bias",
      "img_backbone.blocks.11.attn.q_bias",
      "img_backbone.blocks.11.attn.v_bias",
      "img_backbone.blocks.11.attn.proj.bias",
      "img_backbone.blocks.11.norm2.weight",
      "img_backbone.blocks.11.norm2.bias",
      "img_backbone.blocks.11.mlp.w1.bias",
      "img_backbone.blocks.11.mlp.w2.bias",
      "img_backbone.blocks.11.mlp.ffn_ln.weight",
      "img_backbone.blocks.11.mlp.ffn_ln.bias",
      "img_backbone.blocks.11.mlp.w3.bias"
    ],
    "lr_scale": 0.2541865828329001,
    "lr": 2.0334926626632008e-05,
    "weight_decay": 0.0
  },
  "layer_12_decay": {
    "param_names": [
      "img_backbone.blocks.11.attn.q_proj.weight",
      "img_backbone.blocks.11.attn.k_proj.weight",
      "img_backbone.blocks.11.attn.v_proj.weight",
      "img_backbone.blocks.11.attn.proj.weight",
      "img_backbone.blocks.11.mlp.w1.weight",
      "img_backbone.blocks.11.mlp.w2.weight",
      "img_backbone.blocks.11.mlp.w3.weight"
    ],
    "lr_scale": 0.2541865828329001,
    "lr": 2.0334926626632008e-05,
    "weight_decay": 1e-05
  },
  "layer_13_no_decay": {
    "param_names": [
      "img_backbone.blocks.12.norm1.weight",
      "img_backbone.blocks.12.norm1.bias",
      "img_backbone.blocks.12.attn.q_bias",
      "img_backbone.blocks.12.attn.v_bias",
      "img_backbone.blocks.12.attn.proj.bias",
      "img_backbone.blocks.12.norm2.weight",
      "img_backbone.blocks.12.norm2.bias",
      "img_backbone.blocks.12.mlp.w1.bias",
      "img_backbone.blocks.12.mlp.w2.bias",
      "img_backbone.blocks.12.mlp.ffn_ln.weight",
      "img_backbone.blocks.12.mlp.ffn_ln.bias",
      "img_backbone.blocks.12.mlp.w3.bias"
    ],
    "lr_scale": 0.2824295364810001,
    "lr": 2.259436291848001e-05,
    "weight_decay": 0.0
  },
  "layer_13_decay": {
    "param_names": [
      "img_backbone.blocks.12.attn.q_proj.weight",
      "img_backbone.blocks.12.attn.k_proj.weight",
      "img_backbone.blocks.12.attn.v_proj.weight",
      "img_backbone.blocks.12.attn.proj.weight",
      "img_backbone.blocks.12.mlp.w1.weight",
      "img_backbone.blocks.12.mlp.w2.weight",
      "img_backbone.blocks.12.mlp.w3.weight"
    ],
    "lr_scale": 0.2824295364810001,
    "lr": 2.259436291848001e-05,
    "weight_decay": 1e-05
  },
  "layer_14_no_decay": {
    "param_names": [
      "img_backbone.blocks.13.norm1.weight",
      "img_backbone.blocks.13.norm1.bias",
      "img_backbone.blocks.13.attn.q_bias",
      "img_backbone.blocks.13.attn.v_bias",
      "img_backbone.blocks.13.attn.proj.bias",
      "img_backbone.blocks.13.norm2.weight",
      "img_backbone.blocks.13.norm2.bias",
      "img_backbone.blocks.13.mlp.w1.bias",
      "img_backbone.blocks.13.mlp.w2.bias",
      "img_backbone.blocks.13.mlp.ffn_ln.weight",
      "img_backbone.blocks.13.mlp.ffn_ln.bias",
      "img_backbone.blocks.13.mlp.w3.bias"
    ],
    "lr_scale": 0.31381059609000006,
    "lr": 2.5104847687200008e-05,
    "weight_decay": 0.0
  },
  "layer_14_decay": {
    "param_names": [
      "img_backbone.blocks.13.attn.q_proj.weight",
      "img_backbone.blocks.13.attn.k_proj.weight",
      "img_backbone.blocks.13.attn.v_proj.weight",
      "img_backbone.blocks.13.attn.proj.weight",
      "img_backbone.blocks.13.mlp.w1.weight",
      "img_backbone.blocks.13.mlp.w2.weight",
      "img_backbone.blocks.13.mlp.w3.weight"
    ],
    "lr_scale": 0.31381059609000006,
    "lr": 2.5104847687200008e-05,
    "weight_decay": 1e-05
  },
  "layer_15_no_decay": {
    "param_names": [
      "img_backbone.blocks.14.norm1.weight",
      "img_backbone.blocks.14.norm1.bias",
      "img_backbone.blocks.14.attn.q_bias",
      "img_backbone.blocks.14.attn.v_bias",
      "img_backbone.blocks.14.attn.proj.bias",
      "img_backbone.blocks.14.norm2.weight",
      "img_backbone.blocks.14.norm2.bias",
      "img_backbone.blocks.14.mlp.w1.bias",
      "img_backbone.blocks.14.mlp.w2.bias",
      "img_backbone.blocks.14.mlp.ffn_ln.weight",
      "img_backbone.blocks.14.mlp.ffn_ln.bias",
      "img_backbone.blocks.14.mlp.w3.bias"
    ],
    "lr_scale": 0.3486784401000001,
    "lr": 2.789427520800001e-05,
    "weight_decay": 0.0
  },
  "layer_15_decay": {
    "param_names": [
      "img_backbone.blocks.14.attn.q_proj.weight",
      "img_backbone.blocks.14.attn.k_proj.weight",
      "img_backbone.blocks.14.attn.v_proj.weight",
      "img_backbone.blocks.14.attn.proj.weight",
      "img_backbone.blocks.14.mlp.w1.weight",
      "img_backbone.blocks.14.mlp.w2.weight",
      "img_backbone.blocks.14.mlp.w3.weight"
    ],
    "lr_scale": 0.3486784401000001,
    "lr": 2.789427520800001e-05,
    "weight_decay": 1e-05
  },
  "layer_16_no_decay": {
    "param_names": [
      "img_backbone.blocks.15.norm1.weight",
      "img_backbone.blocks.15.norm1.bias",
      "img_backbone.blocks.15.attn.q_bias",
      "img_backbone.blocks.15.attn.v_bias",
      "img_backbone.blocks.15.attn.proj.bias",
      "img_backbone.blocks.15.norm2.weight",
      "img_backbone.blocks.15.norm2.bias",
      "img_backbone.blocks.15.mlp.w1.bias",
      "img_backbone.blocks.15.mlp.w2.bias",
      "img_backbone.blocks.15.mlp.ffn_ln.weight",
      "img_backbone.blocks.15.mlp.ffn_ln.bias",
      "img_backbone.blocks.15.mlp.w3.bias"
    ],
    "lr_scale": 0.3874204890000001,
    "lr": 3.099363912000001e-05,
    "weight_decay": 0.0
  },
  "layer_16_decay": {
    "param_names": [
      "img_backbone.blocks.15.attn.q_proj.weight",
      "img_backbone.blocks.15.attn.k_proj.weight",
      "img_backbone.blocks.15.attn.v_proj.weight",
      "img_backbone.blocks.15.attn.proj.weight",
      "img_backbone.blocks.15.mlp.w1.weight",
      "img_backbone.blocks.15.mlp.w2.weight",
      "img_backbone.blocks.15.mlp.w3.weight"
    ],
    "lr_scale": 0.3874204890000001,
    "lr": 3.099363912000001e-05,
    "weight_decay": 1e-05
  },
  "layer_17_no_decay": {
    "param_names": [
      "img_backbone.blocks.16.norm1.weight",
      "img_backbone.blocks.16.norm1.bias",
      "img_backbone.blocks.16.attn.q_bias",
      "img_backbone.blocks.16.attn.v_bias",
      "img_backbone.blocks.16.attn.proj.bias",
      "img_backbone.blocks.16.norm2.weight",
      "img_backbone.blocks.16.norm2.bias",
      "img_backbone.blocks.16.mlp.w1.bias",
      "img_backbone.blocks.16.mlp.w2.bias",
      "img_backbone.blocks.16.mlp.ffn_ln.weight",
      "img_backbone.blocks.16.mlp.ffn_ln.bias",
      "img_backbone.blocks.16.mlp.w3.bias"
    ],
    "lr_scale": 0.4304672100000001,
    "lr": 3.443737680000001e-05,
    "weight_decay": 0.0
  },
  "layer_17_decay": {
    "param_names": [
      "img_backbone.blocks.16.attn.q_proj.weight",
      "img_backbone.blocks.16.attn.k_proj.weight",
      "img_backbone.blocks.16.attn.v_proj.weight",
      "img_backbone.blocks.16.attn.proj.weight",
      "img_backbone.blocks.16.mlp.w1.weight",
      "img_backbone.blocks.16.mlp.w2.weight",
      "img_backbone.blocks.16.mlp.w3.weight"
    ],
    "lr_scale": 0.4304672100000001,
    "lr": 3.443737680000001e-05,
    "weight_decay": 1e-05
  },
  "layer_18_no_decay": {
    "param_names": [
      "img_backbone.blocks.17.norm1.weight",
      "img_backbone.blocks.17.norm1.bias",
      "img_backbone.blocks.17.attn.q_bias",
      "img_backbone.blocks.17.attn.v_bias",
      "img_backbone.blocks.17.attn.proj.bias",
      "img_backbone.blocks.17.norm2.weight",
      "img_backbone.blocks.17.norm2.bias",
      "img_backbone.blocks.17.mlp.w1.bias",
      "img_backbone.blocks.17.mlp.w2.bias",
      "img_backbone.blocks.17.mlp.ffn_ln.weight",
      "img_backbone.blocks.17.mlp.ffn_ln.bias",
      "img_backbone.blocks.17.mlp.w3.bias"
    ],
    "lr_scale": 0.4782969000000001,
    "lr": 3.8263752000000007e-05,
    "weight_decay": 0.0
  },
  "layer_18_decay": {
    "param_names": [
      "img_backbone.blocks.17.attn.q_proj.weight",
      "img_backbone.blocks.17.attn.k_proj.weight",
      "img_backbone.blocks.17.attn.v_proj.weight",
      "img_backbone.blocks.17.attn.proj.weight",
      "img_backbone.blocks.17.mlp.w1.weight",
      "img_backbone.blocks.17.mlp.w2.weight",
      "img_backbone.blocks.17.mlp.w3.weight"
    ],
    "lr_scale": 0.4782969000000001,
    "lr": 3.8263752000000007e-05,
    "weight_decay": 1e-05
  },
  "layer_19_no_decay": {
    "param_names": [
      "img_backbone.blocks.18.norm1.weight",
      "img_backbone.blocks.18.norm1.bias",
      "img_backbone.blocks.18.attn.q_bias",
      "img_backbone.blocks.18.attn.v_bias",
      "img_backbone.blocks.18.attn.proj.bias",
      "img_backbone.blocks.18.norm2.weight",
      "img_backbone.blocks.18.norm2.bias",
      "img_backbone.blocks.18.mlp.w1.bias",
      "img_backbone.blocks.18.mlp.w2.bias",
      "img_backbone.blocks.18.mlp.ffn_ln.weight",
      "img_backbone.blocks.18.mlp.ffn_ln.bias",
      "img_backbone.blocks.18.mlp.w3.bias"
    ],
    "lr_scale": 0.531441,
    "lr": 4.251528000000001e-05,
    "weight_decay": 0.0
  },
  "layer_19_decay": {
    "param_names": [
      "img_backbone.blocks.18.attn.q_proj.weight",
      "img_backbone.blocks.18.attn.k_proj.weight",
      "img_backbone.blocks.18.attn.v_proj.weight",
      "img_backbone.blocks.18.attn.proj.weight",
      "img_backbone.blocks.18.mlp.w1.weight",
      "img_backbone.blocks.18.mlp.w2.weight",
      "img_backbone.blocks.18.mlp.w3.weight"
    ],
    "lr_scale": 0.531441,
    "lr": 4.251528000000001e-05,
    "weight_decay": 1e-05
  },
  "layer_20_no_decay": {
    "param_names": [
      "img_backbone.blocks.19.norm1.weight",
      "img_backbone.blocks.19.norm1.bias",
      "img_backbone.blocks.19.attn.q_bias",
      "img_backbone.blocks.19.attn.v_bias",
      "img_backbone.blocks.19.attn.proj.bias",
      "img_backbone.blocks.19.norm2.weight",
      "img_backbone.blocks.19.norm2.bias",
      "img_backbone.blocks.19.mlp.w1.bias",
      "img_backbone.blocks.19.mlp.w2.bias",
      "img_backbone.blocks.19.mlp.ffn_ln.weight",
      "img_backbone.blocks.19.mlp.ffn_ln.bias",
      "img_backbone.blocks.19.mlp.w3.bias"
    ],
    "lr_scale": 0.5904900000000001,
    "lr": 4.723920000000001e-05,
    "weight_decay": 0.0
  },
  "layer_20_decay": {
    "param_names": [
      "img_backbone.blocks.19.attn.q_proj.weight",
      "img_backbone.blocks.19.attn.k_proj.weight",
      "img_backbone.blocks.19.attn.v_proj.weight",
      "img_backbone.blocks.19.attn.proj.weight",
      "img_backbone.blocks.19.mlp.w1.weight",
      "img_backbone.blocks.19.mlp.w2.weight",
      "img_backbone.blocks.19.mlp.w3.weight"
    ],
    "lr_scale": 0.5904900000000001,
    "lr": 4.723920000000001e-05,
    "weight_decay": 1e-05
  },
  "layer_21_no_decay": {
    "param_names": [
      "img_backbone.blocks.20.norm1.weight",
      "img_backbone.blocks.20.norm1.bias",
      "img_backbone.blocks.20.attn.q_bias",
      "img_backbone.blocks.20.attn.v_bias",
      "img_backbone.blocks.20.attn.proj.bias",
      "img_backbone.blocks.20.norm2.weight",
      "img_backbone.blocks.20.norm2.bias",
      "img_backbone.blocks.20.mlp.w1.bias",
      "img_backbone.blocks.20.mlp.w2.bias",
      "img_backbone.blocks.20.mlp.ffn_ln.weight",
      "img_backbone.blocks.20.mlp.ffn_ln.bias",
      "img_backbone.blocks.20.mlp.w3.bias"
    ],
    "lr_scale": 0.6561,
    "lr": 5.2488e-05,
    "weight_decay": 0.0
  },
  "layer_21_decay": {
    "param_names": [
      "img_backbone.blocks.20.attn.q_proj.weight",
      "img_backbone.blocks.20.attn.k_proj.weight",
      "img_backbone.blocks.20.attn.v_proj.weight",
      "img_backbone.blocks.20.attn.proj.weight",
      "img_backbone.blocks.20.mlp.w1.weight",
      "img_backbone.blocks.20.mlp.w2.weight",
      "img_backbone.blocks.20.mlp.w3.weight"
    ],
    "lr_scale": 0.6561,
    "lr": 5.2488e-05,
    "weight_decay": 1e-05
  },
  "layer_22_no_decay": {
    "param_names": [
      "img_backbone.blocks.21.norm1.weight",
      "img_backbone.blocks.21.norm1.bias",
      "img_backbone.blocks.21.attn.q_bias",
      "img_backbone.blocks.21.attn.v_bias",
      "img_backbone.blocks.21.attn.proj.bias",
      "img_backbone.blocks.21.norm2.weight",
      "img_backbone.blocks.21.norm2.bias",
      "img_backbone.blocks.21.mlp.w1.bias",
      "img_backbone.blocks.21.mlp.w2.bias",
      "img_backbone.blocks.21.mlp.ffn_ln.weight",
      "img_backbone.blocks.21.mlp.ffn_ln.bias",
      "img_backbone.blocks.21.mlp.w3.bias"
    ],
    "lr_scale": 0.7290000000000001,
    "lr": 5.832000000000001e-05,
    "weight_decay": 0.0
  },
  "layer_22_decay": {
    "param_names": [
      "img_backbone.blocks.21.attn.q_proj.weight",
      "img_backbone.blocks.21.attn.k_proj.weight",
      "img_backbone.blocks.21.attn.v_proj.weight",
      "img_backbone.blocks.21.attn.proj.weight",
      "img_backbone.blocks.21.mlp.w1.weight",
      "img_backbone.blocks.21.mlp.w2.weight",
      "img_backbone.blocks.21.mlp.w3.weight"
    ],
    "lr_scale": 0.7290000000000001,
    "lr": 5.832000000000001e-05,
    "weight_decay": 1e-05
  },
  "layer_23_no_decay": {
    "param_names": [
      "img_backbone.blocks.22.norm1.weight",
      "img_backbone.blocks.22.norm1.bias",
      "img_backbone.blocks.22.attn.q_bias",
      "img_backbone.blocks.22.attn.v_bias",
      "img_backbone.blocks.22.attn.proj.bias",
      "img_backbone.blocks.22.norm2.weight",
      "img_backbone.blocks.22.norm2.bias",
      "img_backbone.blocks.22.mlp.w1.bias",
      "img_backbone.blocks.22.mlp.w2.bias",
      "img_backbone.blocks.22.mlp.ffn_ln.weight",
      "img_backbone.blocks.22.mlp.ffn_ln.bias",
      "img_backbone.blocks.22.mlp.w3.bias"
    ],
    "lr_scale": 0.81,
    "lr": 6.48e-05,
    "weight_decay": 0.0
  },
  "layer_23_decay": {
    "param_names": [
      "img_backbone.blocks.22.attn.q_proj.weight",
      "img_backbone.blocks.22.attn.k_proj.weight",
      "img_backbone.blocks.22.attn.v_proj.weight",
      "img_backbone.blocks.22.attn.proj.weight",
      "img_backbone.blocks.22.mlp.w1.weight",
      "img_backbone.blocks.22.mlp.w2.weight",
      "img_backbone.blocks.22.mlp.w3.weight"
    ],
    "lr_scale": 0.81,
    "lr": 6.48e-05,
    "weight_decay": 1e-05
  },
  "layer_24_no_decay": {
    "param_names": [
      "img_backbone.blocks.23.norm1.weight",
      "img_backbone.blocks.23.norm1.bias",
      "img_backbone.blocks.23.attn.q_bias",
      "img_backbone.blocks.23.attn.v_bias",
      "img_backbone.blocks.23.attn.proj.bias",
      "img_backbone.blocks.23.norm2.weight",
      "img_backbone.blocks.23.norm2.bias",
      "img_backbone.blocks.23.mlp.w1.bias",
      "img_backbone.blocks.23.mlp.w2.bias",
      "img_backbone.blocks.23.mlp.ffn_ln.weight",
      "img_backbone.blocks.23.mlp.ffn_ln.bias",
      "img_backbone.blocks.23.mlp.w3.bias"
    ],
    "lr_scale": 0.9,
    "lr": 7.2e-05,
    "weight_decay": 0.0
  },
  "layer_24_decay": {
    "param_names": [
      "img_backbone.blocks.23.attn.q_proj.weight",
      "img_backbone.blocks.23.attn.k_proj.weight",
      "img_backbone.blocks.23.attn.v_proj.weight",
      "img_backbone.blocks.23.attn.proj.weight",
      "img_backbone.blocks.23.mlp.w1.weight",
      "img_backbone.blocks.23.mlp.w2.weight",
      "img_backbone.blocks.23.mlp.w3.weight"
    ],
    "lr_scale": 0.9,
    "lr": 7.2e-05,
    "weight_decay": 1e-05
  }
}
/mnt/sdb/swseo/Orion/mmcv/runner/hooks/optimizer.py:173: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.loss_scaler = GradScaler()
2026-01-13 18:37:51,605 - 0.0.1 - INFO - Start running, host: root@LearningMachine3, work_dir: /mnt/sdb/swseo/Orion/adzoo/orion/work_dirs/orion_stage2_train
2026-01-13 18:37:51,606 - 0.0.1 - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(NORMAL      ) CustomDistEvalHook                 
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CustomDistEvalHook                 
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_train_iter:
(VERY_HIGH   ) CosineAnnealingLrUpdaterHook       
(NORMAL      ) CustomDistEvalHook                 
(LOW         ) IterTimerHook                      
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) Fp16OptimizerHook                  
(NORMAL      ) CheckpointHook                     
(NORMAL      ) CustomDistEvalHook                 
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(NORMAL      ) CustomDistEvalHook                 
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
(VERY_LOW    ) TensorboardLoggerHook              
 -------------------- 
2026-01-13 18:37:51,607 - 0.0.1 - INFO - workflow: [('train', 1)], max: 44016 iters
2026-01-13 18:37:51,615 - 0.0.1 - INFO - Checkpoints will be saved to /mnt/sdb/swseo/Orion/adzoo/orion/work_dirs/orion_stage2_train by HardDiskBackend.
/mnt/sdb/swseo/Orion/mmcv/utils/fp16_utils.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=True):
/mnt/sdb/swseo/Orion/mmcv/models/utils/grid_mask.py:114: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)
  mask = torch.from_numpy(mask).to(x.dtype).cuda()
/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/mnt/sdb/swseo/Orion/mmcv/utils/fp16_utils.py:211: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=False):
[rank0]: Traceback (most recent call last):
[rank0]:   File "./adzoo/orion/train.py", line 238, in <module>
[rank0]:     main()
[rank0]:   File "./adzoo/orion/train.py", line 226, in main
[rank0]:     custom_train_model(
[rank0]:   File "/mnt/sdb/swseo/Orion/adzoo/orion/apis/train.py", line 19, in custom_train_model
[rank0]:     custom_train_detector(
[rank0]:   File "/mnt/sdb/swseo/Orion/adzoo/orion/apis/mmdet_train.py", line 190, in custom_train_detector
[rank0]:     runner.run(data_loaders, cfg.workflow)
[rank0]:   File "/mnt/sdb/swseo/Orion/mmcv/runner/iter_based_runner.py", line 144, in run
[rank0]:     iter_runner(iter_loaders[i], **kwargs)
[rank0]:   File "/mnt/sdb/swseo/Orion/mmcv/runner/iter_based_runner.py", line 64, in train
[rank0]:     outputs = self.model(data_batch, return_loss=True, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/Orion/mmcv/models/detectors/orion.py", line 435, in forward
[rank0]:     losses = self.forward_train(**data)
[rank0]:   File "/mnt/sdb/swseo/Orion/mmcv/models/detectors/orion.py", line 501, in forward_train
[rank0]:     losses = self.forward_pts_train(gt_bboxes_3d, gt_labels_3d, gt_attr_labels,map_gt_bboxes_3d, map_gt_labels_3d, img_metas,input_ids, vlm_labels, vlm_attn_mask, ego_fut_trajs,**data)
[rank0]:   File "/mnt/sdb/swseo/Orion/mmcv/models/detectors/orion.py", line 586, in forward_pts_train
[rank0]:     sample, output_distribution = self.distribution_forward(
[rank0]:   File "/mnt/sdb/swseo/Orion/mmcv/models/detectors/orion.py", line 1641, in distribution_forward
[rank0]:     present_mu, present_log_sigma = self.present_distribution(present_features)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/Orion/mmcv/models/utils/distributions.py", line 37, in forward
[rank0]:     encoding = self.encoder(s_t.permute(0, 2, 1).float())
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/Orion/mmcv/models/utils/distributions.py", line 90, in forward
[rank0]:     s_t = self.relu(self.conv1(s_t))
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 308, in forward
[rank0]:     return self._conv_forward(input, self.weight, self.bias)
[rank0]:   File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 304, in _conv_forward
[rank0]:     return F.conv1d(input, weight, bias, self.stride,
[rank0]: RuntimeError: Given groups=1, weight of size [8192, 4096, 1], expected input[1, 2048, 1] to have 4096 channels, but got 2048 channels instead
/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
E0113 18:38:28.942927 140057448478528 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 13039) of binary: /mnt/sdb/swseo/orion_docker/envs/orion/bin/python
Traceback (most recent call last):
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launch.py", line 208, in <module>
    main()
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/typing_extensions.py", line 2853, in wrapper
    return arg(*args, **kwargs)
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launch.py", line 204, in main
    launch(args)
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in launch
    run(args)
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/sdb/swseo/orion_docker/envs/orion/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./adzoo/orion/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-13_18:38:28
  host      : LearningMachine3
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 13039)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
